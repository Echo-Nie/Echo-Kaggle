{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import BRICS\n",
    "from rdkit.Chem.EnumerateStereoisomers import EnumerateStereoisomers, StereoEnumerationOptions\n",
    "import random\n",
    "from typing import Optional, List, Union, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "def augment_smiles_dataset(df: pd.DataFrame,\n",
    "                          smiles_column: str = 'SMILES',\n",
    "                          augmentation_strategies: List[str] = ['enumeration', 'kekulize', 'stereo_enum', 'fragment', 'rotate'],\n",
    "                          n_augmentations: int = 10,  # 减少增强次数，提高质量\n",
    "                          preserve_original: bool = True,\n",
    "                          random_seed: Optional[int] = None,\n",
    "                          max_fragment_combinations: int = 2,\n",
    "                          verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    增强的SMILES数据集生成器\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    def debug_print(msg: str):\n",
    "        if verbose:\n",
    "            print(msg)\n",
    "    \n",
    "    def sanitize_smiles(smiles: str) -> Optional[str]:\n",
    "        \"\"\"净化并验证SMILES字符串\"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                return Chem.MolToSmiles(mol, canonical=True)\n",
    "        except Exception as e:\n",
    "            debug_print(f\"SMILES净化失败: {smiles}, 错误: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    def apply_augmentation_strategy(smiles: str, strategy: str) -> List[str]:\n",
    "        debug_print(f\"\\n尝试应用{strategy}策略到SMILES: {smiles}\")\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                debug_print(f\"无法从SMILES创建分子: {smiles}\")\n",
    "                return []\n",
    "            \n",
    "            augmented = set()\n",
    "            \n",
    "            if strategy == 'enumeration':\n",
    "                # SMILES枚举\n",
    "                for _ in range(n_augmentations):\n",
    "                    try:\n",
    "                        enum_smiles = Chem.MolToSmiles(mol, \n",
    "                                                     canonical=False, \n",
    "                                                     doRandom=True,\n",
    "                                                     isomericSmiles=True)\n",
    "                        sanitized = sanitize_smiles(enum_smiles)\n",
    "                        if sanitized and sanitized != smiles:\n",
    "                            augmented.add(sanitized)\n",
    "                    except Exception as e:\n",
    "                        debug_print(f\"枚举失败: {str(e)}\")\n",
    "            \n",
    "            elif strategy == 'kekulize':\n",
    "                # Kekulization变体\n",
    "                try:\n",
    "                    Chem.Kekulize(mol)\n",
    "                    kek_smiles = Chem.MolToSmiles(mol, kekuleSmiles=True)\n",
    "                    sanitized = sanitize_smiles(kek_smiles)\n",
    "                    if sanitized and sanitized != smiles:\n",
    "                        augmented.add(sanitized)\n",
    "                    \n",
    "                    # 添加带有显式芳香性的版本\n",
    "                    aromatic_smiles = Chem.MolToSmiles(mol, kekuleSmiles=False)\n",
    "                    sanitized = sanitize_smiles(aromatic_smiles)\n",
    "                    if sanitized and sanitized != smiles:\n",
    "                        augmented.add(sanitized)\n",
    "                except Exception as e:\n",
    "                    debug_print(f\"Kekulization失败: {str(e)}\")\n",
    "            \n",
    "            elif strategy == 'stereo_enum':\n",
    "                try:\n",
    "                    # 使用RDKit的立体异构体枚举器\n",
    "                    opts = StereoEnumerationOptions(unique=True)\n",
    "                    isomers = tuple(EnumerateStereoisomers(mol, options=opts))\n",
    "                    for isomer in isomers:\n",
    "                        iso_smiles = Chem.MolToSmiles(isomer, isomericSmiles=True)\n",
    "                        sanitized = sanitize_smiles(iso_smiles)\n",
    "                        if sanitized and sanitized != smiles:\n",
    "                            augmented.add(sanitized)\n",
    "                except Exception as e:\n",
    "                    debug_print(f\"立体异构体枚举失败: {str(e)}\")\n",
    "            \n",
    "            elif strategy == 'fragment':\n",
    "                try:\n",
    "                    # BRICS分解和重组\n",
    "                    fragments = list(BRICS.BRICSDecompose(mol))\n",
    "                    debug_print(f\"BRICS分解得到{len(fragments)}个片段\")\n",
    "                    if len(fragments) > 1:\n",
    "                        for _ in range(min(max_fragment_combinations, n_augmentations)):\n",
    "                            try:\n",
    "                                n_frags = random.randint(2, min(len(fragments), 3))\n",
    "                                selected_frags = random.sample(fragments, n_frags)\n",
    "                                new_mol = BRICS.BRICSBuild([Chem.MolFromSmiles(f) for f in selected_frags])\n",
    "                                if new_mol:\n",
    "                                    new_smiles = Chem.MolToSmiles(new_mol)\n",
    "                                    sanitized = sanitize_smiles(new_smiles)\n",
    "                                    if sanitized and sanitized != smiles:\n",
    "                                        augmented.add(sanitized)\n",
    "                            except Exception as e:\n",
    "                                debug_print(f\"片段重组失败: {str(e)}\")\n",
    "                except Exception as e:\n",
    "                    debug_print(f\"BRICS分解失败: {str(e)}\")\n",
    "            \n",
    "            elif strategy == 'rotate':\n",
    "                try:\n",
    "                    for i in range(mol.GetNumAtoms()):\n",
    "                        try:\n",
    "                            rot_smiles = Chem.MolToSmiles(mol, \n",
    "                                                         doRandom=True,\n",
    "                                                         canonical=False,\n",
    "                                                         rootedAtAtom=i)\n",
    "                            sanitized = sanitize_smiles(rot_smiles)\n",
    "                            if sanitized and sanitized != smiles:\n",
    "                                augmented.add(sanitized)\n",
    "                        except Exception as e:\n",
    "                            debug_print(f\"原子{i}旋转失败: {str(e)}\")\n",
    "                except Exception as e:\n",
    "                    debug_print(f\"分子旋转失败: {str(e)}\")\n",
    "            \n",
    "            result = list(augmented)\n",
    "            debug_print(f\"{strategy}策略生成了{len(result)}个新SMILES\")\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            debug_print(f\"策略{strategy}整体失败: {str(e)}\")\n",
    "            return []\n",
    "    \n",
    "    augmented_rows = []\n",
    "    stats: Dict[str, int] = {strategy: 0 for strategy in augmentation_strategies}\n",
    "    stats['original'] = 0\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"增强SMILES\"):\n",
    "        original_smiles = row[smiles_column]\n",
    "        debug_print(f\"\\n处理SMILES {idx+1}/{len(df)}: {original_smiles}\")\n",
    "        \n",
    "        if preserve_original:\n",
    "            original_row = row.to_dict()\n",
    "            original_row['augmentation_strategy'] = 'original'\n",
    "            original_row['is_augmented'] = False\n",
    "            augmented_rows.append(original_row)\n",
    "            stats['original'] += 1\n",
    "        \n",
    "        for strategy in augmentation_strategies:\n",
    "            strategy_smiles = apply_augmentation_strategy(original_smiles, strategy)\n",
    "            \n",
    "            for aug_smiles in strategy_smiles:\n",
    "                if aug_smiles != original_smiles:\n",
    "                    new_row = row.to_dict().copy()\n",
    "                    new_row[smiles_column] = aug_smiles\n",
    "                    new_row['augmentation_strategy'] = strategy\n",
    "                    new_row['is_augmented'] = True\n",
    "                    augmented_rows.append(new_row)\n",
    "                    stats[strategy] += 1\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    augmented_df = augmented_df.reset_index(drop=True)\n",
    "    \n",
    "    # 打印详细统计信息\n",
    "    print(\"\\n增强结果统计:\")\n",
    "    print(f\"原始数据集大小: {len(df)}\")\n",
    "    print(f\"增强后数据集大小: {len(augmented_df)}\")\n",
    "    print(f\"总体增强倍数: {len(augmented_df) / len(df):.2f}x\")\n",
    "    print(\"\\n各策略贡献:\")\n",
    "    for strategy, count in stats.items():\n",
    "        print(f\"- {strategy}: {count}条记录 ({count/len(df):.2f}x)\")\n",
    "    \n",
    "    return augmented_df\n",
    "\n",
    "# 让我们看看test数据的内容\n",
    "print(\"原始test数据的前几行:\")\n",
    "print(test.head())\n",
    "print(\"\\n开始数据增强...\")\n",
    "test = augment_smiles_dataset(test, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import BRICS\n",
    "import random\n",
    "from typing import Optional, List, Union, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "def augment_smiles_dataset(df: pd.DataFrame,\n",
    "                          smiles_column: str = 'SMILES',\n",
    "                          augmentation_strategies: List[str] = ['enumeration', 'kekulize', 'stereo_enum', 'fragment', 'rotate'],\n",
    "                          n_augmentations: int = 100,\n",
    "                          preserve_original: bool = True,\n",
    "                          random_seed: Optional[int] = None,\n",
    "                          max_fragment_combinations: int = 2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    增强的SMILES数据集生成器\n",
    "    \n",
    "    参数:\n",
    "    - df: 输入的DataFrame\n",
    "    - smiles_column: SMILES列的名称\n",
    "    - augmentation_strategies: 增强策略列表\n",
    "    - n_augmentations: 每个策略的增强次数\n",
    "    - preserve_original: 是否保留原始SMILES\n",
    "    - random_seed: 随机种子\n",
    "    - max_fragment_combinations: 片段重组的最大组合数\n",
    "    \n",
    "    返回:\n",
    "    - 增强后的DataFrame\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    def sanitize_smiles(smiles: str) -> str:\n",
    "        \"\"\"净化SMILES字符串\"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                return Chem.MolToSmiles(mol, canonical=True)\n",
    "        except:\n",
    "            pass\n",
    "        return smiles\n",
    "    \n",
    "    def apply_augmentation_strategy(smiles: str, strategy: str) -> List[str]:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                return [smiles]\n",
    "            \n",
    "            augmented = set()  # 使用集合避免重复\n",
    "            \n",
    "            if strategy == 'enumeration':\n",
    "                # 增强的SMILES枚举\n",
    "                for _ in range(n_augmentations):\n",
    "                    enum_smiles = Chem.MolToSmiles(mol, \n",
    "                                                 canonical=False, \n",
    "                                                 doRandom=True,\n",
    "                                                 isomericSmiles=True)\n",
    "                    augmented.add(sanitize_smiles(enum_smiles))\n",
    "            \n",
    "            elif strategy == 'kekulize':\n",
    "                # Kekulization变体\n",
    "                try:\n",
    "                    Chem.Kekulize(mol)\n",
    "                    kek_smiles = Chem.MolToSmiles(mol, kekuleSmiles=True)\n",
    "                    augmented.add(sanitize_smiles(kek_smiles))\n",
    "                    \n",
    "                    # 添加不同的Kekulization形式\n",
    "                    for _ in range(min(5, n_augmentations)):\n",
    "                        new_mol = Chem.MolFromSmiles(kek_smiles)\n",
    "                        if new_mol:\n",
    "                            Chem.Kekulize(new_mol)\n",
    "                            new_smiles = Chem.MolToSmiles(new_mol, kekuleSmiles=True, doRandom=True)\n",
    "                            augmented.add(sanitize_smiles(new_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'stereo_enum':\n",
    "                # 立体化学枚举\n",
    "                try:\n",
    "                    # 移除立体化学\n",
    "                    Chem.RemoveStereochemistry(mol)\n",
    "                    no_stereo = Chem.MolToSmiles(mol)\n",
    "                    augmented.add(sanitize_smiles(no_stereo))\n",
    "                    \n",
    "                    # 添加随机立体中心\n",
    "                    for _ in range(n_augmentations // 2):\n",
    "                        new_mol = Chem.MolFromSmiles(no_stereo)\n",
    "                        if new_mol:\n",
    "                            Chem.AssignStereochemistry(new_mol, force=True, cleanIt=True)\n",
    "                            stereo_smiles = Chem.MolToSmiles(new_mol, isomericSmiles=True)\n",
    "                            augmented.add(sanitize_smiles(stereo_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'fragment':\n",
    "                # BRICS分解和重组\n",
    "                try:\n",
    "                    fragments = list(BRICS.BRICSDecompose(mol))\n",
    "                    if len(fragments) > 1:\n",
    "                        # 随机组合片段\n",
    "                        for _ in range(min(max_fragment_combinations, n_augmentations)):\n",
    "                            n_frags = random.randint(2, min(len(fragments), 3))\n",
    "                            selected_frags = random.sample(fragments, n_frags)\n",
    "                            new_mol = BRICS.BRICSBuild([Chem.MolFromSmiles(f) for f in selected_frags])\n",
    "                            if new_mol:\n",
    "                                new_smiles = Chem.MolToSmiles(new_mol)\n",
    "                                augmented.add(sanitize_smiles(new_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'rotate':\n",
    "                # 原子顺序旋转\n",
    "                try:\n",
    "                    for _ in range(n_augmentations // 2):\n",
    "                        rot_smiles = Chem.MolToSmiles(mol, \n",
    "                                                     doRandom=True,\n",
    "                                                     canonical=False,\n",
    "                                                     rootedAtAtom=random.randint(0, mol.GetNumAtoms()-1))\n",
    "                        augmented.add(sanitize_smiles(rot_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return list(augmented)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"警告: {strategy}处理{smiles}时出错: {e}\")\n",
    "            return [smiles]\n",
    "    \n",
    "    augmented_rows = []\n",
    "    stats: Dict[str, int] = {strategy: 0 for strategy in augmentation_strategies}\n",
    "    stats['original'] = 0\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"增强SMILES\"):\n",
    "        original_smiles = row[smiles_column]\n",
    "        \n",
    "        if preserve_original:\n",
    "            original_row = row.to_dict()\n",
    "            original_row['augmentation_strategy'] = 'original'\n",
    "            original_row['is_augmented'] = False\n",
    "            augmented_rows.append(original_row)\n",
    "            stats['original'] += 1\n",
    "        \n",
    "        for strategy in augmentation_strategies:\n",
    "            strategy_smiles = apply_augmentation_strategy(original_smiles, strategy)\n",
    "            \n",
    "            for aug_smiles in strategy_smiles:\n",
    "                if aug_smiles != original_smiles:\n",
    "                    new_row = row.to_dict().copy()\n",
    "                    new_row[smiles_column] = aug_smiles\n",
    "                    new_row['augmentation_strategy'] = strategy\n",
    "                    new_row['is_augmented'] = True\n",
    "                    augmented_rows.append(new_row)\n",
    "                    stats[strategy] += 1\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    augmented_df = augmented_df.reset_index(drop=True)\n",
    "    \n",
    "    # 打印详细统计信息\n",
    "    print(\"\\n增强结果统计:\")\n",
    "    print(f\"原始数据集大小: {len(df)}\")\n",
    "    print(f\"增强后数据集大小: {len(augmented_df)}\")\n",
    "    print(f\"总体增强倍数: {len(augmented_df) / len(df):.2f}x\")\n",
    "    print(\"\\n各策略贡献:\")\n",
    "    for strategy, count in stats.items():\n",
    "        print(f\"- {strategy}: {count}条记录 ({count/len(df):.2f}x)\")\n",
    "    \n",
    "    return augmented_df\n",
    "\n",
    "test = augment_smiles_dataset(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import BRICS\n",
    "import random\n",
    "from typing import Optional, List, Union, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "def augment_smiles_dataset(df: pd.DataFrame,\n",
    "                          smiles_column: str = 'SMILES',\n",
    "                          augmentation_strategies: List[str] = ['enumeration', 'kekulize', 'stereo_enum', 'fragment', 'rotate'],\n",
    "                          n_augmentations: int = 100,\n",
    "                          preserve_original: bool = True,\n",
    "                          random_seed: Optional[int] = None,\n",
    "                          max_fragment_combinations: int = 2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    增强的SMILES数据集生成器\n",
    "    \n",
    "    参数:\n",
    "    - df: 输入的DataFrame\n",
    "    - smiles_column: SMILES列的名称\n",
    "    - augmentation_strategies: 增强策略列表\n",
    "    - n_augmentations: 每个策略的增强次数\n",
    "    - preserve_original: 是否保留原始SMILES\n",
    "    - random_seed: 随机种子\n",
    "    - max_fragment_combinations: 片段重组的最大组合数\n",
    "    \n",
    "    返回:\n",
    "    - 增强后的DataFrame\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    def sanitize_smiles(smiles: str) -> str:\n",
    "        \"\"\"净化SMILES字符串\"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                return Chem.MolToSmiles(mol, canonical=True)\n",
    "        except:\n",
    "            pass\n",
    "        return smiles\n",
    "    \n",
    "    def apply_augmentation_strategy(smiles: str, strategy: str) -> List[str]:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                return [smiles]\n",
    "            \n",
    "            augmented = set()  # 使用集合避免重复\n",
    "            \n",
    "            if strategy == 'enumeration':\n",
    "                # 增强的SMILES枚举\n",
    "                for _ in range(n_augmentations):\n",
    "                    enum_smiles = Chem.MolToSmiles(mol, \n",
    "                                                 canonical=False, \n",
    "                                                 doRandom=True,\n",
    "                                                 isomericSmiles=True)\n",
    "                    augmented.add(sanitize_smiles(enum_smiles))\n",
    "            \n",
    "            elif strategy == 'kekulize':\n",
    "                # Kekulization变体\n",
    "                try:\n",
    "                    Chem.Kekulize(mol)\n",
    "                    kek_smiles = Chem.MolToSmiles(mol, kekuleSmiles=True)\n",
    "                    augmented.add(sanitize_smiles(kek_smiles))\n",
    "                    \n",
    "                    # 添加不同的Kekulization形式\n",
    "                    for _ in range(min(5, n_augmentations)):\n",
    "                        new_mol = Chem.MolFromSmiles(kek_smiles)\n",
    "                        if new_mol:\n",
    "                            Chem.Kekulize(new_mol)\n",
    "                            new_smiles = Chem.MolToSmiles(new_mol, kekuleSmiles=True, doRandom=True)\n",
    "                            augmented.add(sanitize_smiles(new_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'stereo_enum':\n",
    "                # 立体化学枚举\n",
    "                try:\n",
    "                    # 移除立体化学\n",
    "                    Chem.RemoveStereochemistry(mol)\n",
    "                    no_stereo = Chem.MolToSmiles(mol)\n",
    "                    augmented.add(sanitize_smiles(no_stereo))\n",
    "                    \n",
    "                    # 添加随机立体中心\n",
    "                    for _ in range(n_augmentations // 2):\n",
    "                        new_mol = Chem.MolFromSmiles(no_stereo)\n",
    "                        if new_mol:\n",
    "                            Chem.AssignStereochemistry(new_mol, force=True, cleanIt=True)\n",
    "                            stereo_smiles = Chem.MolToSmiles(new_mol, isomericSmiles=True)\n",
    "                            augmented.add(sanitize_smiles(stereo_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'fragment':\n",
    "                # BRICS分解和重组\n",
    "                try:\n",
    "                    fragments = list(BRICS.BRICSDecompose(mol))\n",
    "                    if len(fragments) > 1:\n",
    "                        # 随机组合片段\n",
    "                        for _ in range(min(max_fragment_combinations, n_augmentations)):\n",
    "                            n_frags = random.randint(2, min(len(fragments), 3))\n",
    "                            selected_frags = random.sample(fragments, n_frags)\n",
    "                            new_mol = BRICS.BRICSBuild([Chem.MolFromSmiles(f) for f in selected_frags])\n",
    "                            if new_mol:\n",
    "                                new_smiles = Chem.MolToSmiles(new_mol)\n",
    "                                augmented.add(sanitize_smiles(new_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'rotate':\n",
    "                # 原子顺序旋转\n",
    "                try:\n",
    "                    for _ in range(n_augmentations // 2):\n",
    "                        rot_smiles = Chem.MolToSmiles(mol, \n",
    "                                                     doRandom=True,\n",
    "                                                     canonical=False,\n",
    "                                                     rootedAtAtom=random.randint(0, mol.GetNumAtoms()-1))\n",
    "                        augmented.add(sanitize_smiles(rot_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return list(augmented)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"警告: {strategy}处理{smiles}时出错: {e}\")\n",
    "            return [smiles]\n",
    "    \n",
    "    augmented_rows = []\n",
    "    stats: Dict[str, int] = {strategy: 0 for strategy in augmentation_strategies}\n",
    "    stats['original'] = 0\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"增强SMILES\"):\n",
    "        original_smiles = row[smiles_column]\n",
    "        \n",
    "        if preserve_original:\n",
    "            original_row = row.to_dict()\n",
    "            original_row['augmentation_strategy'] = 'original'\n",
    "            original_row['is_augmented'] = False\n",
    "            augmented_rows.append(original_row)\n",
    "            stats['original'] += 1\n",
    "        \n",
    "        for strategy in augmentation_strategies:\n",
    "            strategy_smiles = apply_augmentation_strategy(original_smiles, strategy)\n",
    "            \n",
    "            for aug_smiles in strategy_smiles:\n",
    "                if aug_smiles != original_smiles:\n",
    "                    new_row = row.to_dict().copy()\n",
    "                    new_row[smiles_column] = aug_smiles\n",
    "                    new_row['augmentation_strategy'] = strategy\n",
    "                    new_row['is_augmented'] = True\n",
    "                    augmented_rows.append(new_row)\n",
    "                    stats[strategy] += 1\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    augmented_df = augmented_df.reset_index(drop=True)\n",
    "    \n",
    "    # 打印详细统计信息\n",
    "    print(\"\\n增强结果统计:\")\n",
    "    print(f\"原始数据集大小: {len(df)}\")\n",
    "    print(f\"增强后数据集大小: {len(augmented_df)}\")\n",
    "    print(f\"总体增强倍数: {len(augmented_df) / len(df):.2f}x\")\n",
    "    print(\"\\n各策略贡献:\")\n",
    "    for strategy, count in stats.items():\n",
    "        print(f\"- {strategy}: {count}条记录 ({count/len(df):.2f}x)\")\n",
    "    \n",
    "    return augmented_df\n",
    "\n",
    "test = augment_smiles_dataset(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import BRICS\n",
    "import random\n",
    "from typing import Optional, List, Union, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "def augment_smiles_dataset(df: pd.DataFrame,\n",
    "                          smiles_column: str = 'SMILES',\n",
    "                          augmentation_strategies: List[str] = ['enumeration', 'kekulize', 'stereo_enum', 'fragment', 'rotate'],\n",
    "                          n_augmentations: int = 100,\n",
    "                          preserve_original: bool = True,\n",
    "                          random_seed: Optional[int] = None,\n",
    "                          max_fragment_combinations: int = 2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    增强的SMILES数据集生成器\n",
    "    \n",
    "    参数:\n",
    "    - df: 输入的DataFrame\n",
    "    - smiles_column: SMILES列的名称\n",
    "    - augmentation_strategies: 增强策略列表\n",
    "    - n_augmentations: 每个策略的增强次数\n",
    "    - preserve_original: 是否保留原始SMILES\n",
    "    - random_seed: 随机种子\n",
    "    - max_fragment_combinations: 片段重组的最大组合数\n",
    "    \n",
    "    返回:\n",
    "    - 增强后的DataFrame\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    def sanitize_smiles(smiles: str) -> str:\n",
    "        \"\"\"净化SMILES字符串\"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                return Chem.MolToSmiles(mol, canonical=True)\n",
    "        except:\n",
    "            pass\n",
    "        return smiles\n",
    "    \n",
    "    def apply_augmentation_strategy(smiles: str, strategy: str) -> List[str]:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                return [smiles]\n",
    "            \n",
    "            augmented = set()  # 使用集合避免重复\n",
    "            \n",
    "            if strategy == 'enumeration':\n",
    "                # 增强的SMILES枚举\n",
    "                for _ in range(n_augmentations):\n",
    "                    enum_smiles = Chem.MolToSmiles(mol, \n",
    "                                                 canonical=False, \n",
    "                                                 doRandom=True,\n",
    "                                                 isomericSmiles=True)\n",
    "                    augmented.add(sanitize_smiles(enum_smiles))\n",
    "            \n",
    "            elif strategy == 'kekulize':\n",
    "                # Kekulization变体\n",
    "                try:\n",
    "                    Chem.Kekulize(mol)\n",
    "                    kek_smiles = Chem.MolToSmiles(mol, kekuleSmiles=True)\n",
    "                    augmented.add(sanitize_smiles(kek_smiles))\n",
    "                    \n",
    "                    # 添加不同的Kekulization形式\n",
    "                    for _ in range(min(5, n_augmentations)):\n",
    "                        new_mol = Chem.MolFromSmiles(kek_smiles)\n",
    "                        if new_mol:\n",
    "                            Chem.Kekulize(new_mol)\n",
    "                            new_smiles = Chem.MolToSmiles(new_mol, kekuleSmiles=True, doRandom=True)\n",
    "                            augmented.add(sanitize_smiles(new_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'stereo_enum':\n",
    "                # 立体化学枚举\n",
    "                try:\n",
    "                    # 移除立体化学\n",
    "                    Chem.RemoveStereochemistry(mol)\n",
    "                    no_stereo = Chem.MolToSmiles(mol)\n",
    "                    augmented.add(sanitize_smiles(no_stereo))\n",
    "                    \n",
    "                    # 添加随机立体中心\n",
    "                    for _ in range(n_augmentations // 2):\n",
    "                        new_mol = Chem.MolFromSmiles(no_stereo)\n",
    "                        if new_mol:\n",
    "                            Chem.AssignStereochemistry(new_mol, force=True, cleanIt=True)\n",
    "                            stereo_smiles = Chem.MolToSmiles(new_mol, isomericSmiles=True)\n",
    "                            augmented.add(sanitize_smiles(stereo_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'fragment':\n",
    "                # BRICS分解和重组\n",
    "                try:\n",
    "                    fragments = list(BRICS.BRICSDecompose(mol))\n",
    "                    if len(fragments) > 1:\n",
    "                        # 随机组合片段\n",
    "                        for _ in range(min(max_fragment_combinations, n_augmentations)):\n",
    "                            n_frags = random.randint(2, min(len(fragments), 3))\n",
    "                            selected_frags = random.sample(fragments, n_frags)\n",
    "                            new_mol = BRICS.BRICSBuild([Chem.MolFromSmiles(f) for f in selected_frags])\n",
    "                            if new_mol:\n",
    "                                new_smiles = Chem.MolToSmiles(new_mol)\n",
    "                                augmented.add(sanitize_smiles(new_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'rotate':\n",
    "                # 原子顺序旋转\n",
    "                try:\n",
    "                    for _ in range(n_augmentations // 2):\n",
    "                        rot_smiles = Chem.MolToSmiles(mol, \n",
    "                                                     doRandom=True,\n",
    "                                                     canonical=False,\n",
    "                                                     rootedAtAtom=random.randint(0, mol.GetNumAtoms()-1))\n",
    "                        augmented.add(sanitize_smiles(rot_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return list(augmented)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"警告: {strategy}处理{smiles}时出错: {e}\")\n",
    "            return [smiles]\n",
    "    \n",
    "    augmented_rows = []\n",
    "    stats: Dict[str, int] = {strategy: 0 for strategy in augmentation_strategies}\n",
    "    stats['original'] = 0\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"增强SMILES\"):\n",
    "        original_smiles = row[smiles_column]\n",
    "        \n",
    "        if preserve_original:\n",
    "            original_row = row.to_dict()\n",
    "            original_row['augmentation_strategy'] = 'original'\n",
    "            original_row['is_augmented'] = False\n",
    "            augmented_rows.append(original_row)\n",
    "            stats['original'] += 1\n",
    "        \n",
    "        for strategy in augmentation_strategies:\n",
    "            strategy_smiles = apply_augmentation_strategy(original_smiles, strategy)\n",
    "            \n",
    "            for aug_smiles in strategy_smiles:\n",
    "                if aug_smiles != original_smiles:\n",
    "                    new_row = row.to_dict().copy()\n",
    "                    new_row[smiles_column] = aug_smiles\n",
    "                    new_row['augmentation_strategy'] = strategy\n",
    "                    new_row['is_augmented'] = True\n",
    "                    augmented_rows.append(new_row)\n",
    "                    stats[strategy] += 1\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    augmented_df = augmented_df.reset_index(drop=True)\n",
    "    \n",
    "    # 打印详细统计信息\n",
    "    print(\"\\n增强结果统计:\")\n",
    "    print(f\"原始数据集大小: {len(df)}\")\n",
    "    print(f\"增强后数据集大小: {len(augmented_df)}\")\n",
    "    print(f\"总体增强倍数: {len(augmented_df) / len(df):.2f}x\")\n",
    "    print(\"\\n各策略贡献:\")\n",
    "    for strategy, count in stats.items():\n",
    "        print(f\"- {strategy}: {count}条记录 ({count/len(df):.2f}x)\")\n",
    "    \n",
    "    return augmented_df\n",
    "\n",
    "test = augment_smiles_dataset(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import BRICS\n",
    "import random\n",
    "from typing import Optional, List, Union, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "def augment_smiles_dataset(df: pd.DataFrame,\n",
    "                          smiles_column: str = 'SMILES',\n",
    "                          augmentation_strategies: List[str] = ['enumeration', 'kekulize', 'stereo_enum', 'fragment', 'rotate'],\n",
    "                          n_augmentations: int = 100,\n",
    "                          preserve_original: bool = True,\n",
    "                          random_seed: Optional[int] = None,\n",
    "                          max_fragment_combinations: int = 2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    增强的SMILES数据集生成器\n",
    "    \n",
    "    参数:\n",
    "    - df: 输入的DataFrame\n",
    "    - smiles_column: SMILES列的名称\n",
    "    - augmentation_strategies: 增强策略列表\n",
    "    - n_augmentations: 每个策略的增强次数\n",
    "    - preserve_original: 是否保留原始SMILES\n",
    "    - random_seed: 随机种子\n",
    "    - max_fragment_combinations: 片段重组的最大组合数\n",
    "    \n",
    "    返回:\n",
    "    - 增强后的DataFrame\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    def sanitize_smiles(smiles: str) -> str:\n",
    "        \"\"\"净化SMILES字符串\"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                return Chem.MolToSmiles(mol, canonical=True)\n",
    "        except:\n",
    "            pass\n",
    "        return smiles\n",
    "    \n",
    "    def apply_augmentation_strategy(smiles: str, strategy: str) -> List[str]:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                return [smiles]\n",
    "            \n",
    "            augmented = set()  # 使用集合避免重复\n",
    "            \n",
    "            if strategy == 'enumeration':\n",
    "                # 增强的SMILES枚举\n",
    "                for _ in range(n_augmentations):\n",
    "                    enum_smiles = Chem.MolToSmiles(mol, \n",
    "                                                 canonical=False, \n",
    "                                                 doRandom=True,\n",
    "                                                 isomericSmiles=True)\n",
    "                    augmented.add(sanitize_smiles(enum_smiles))\n",
    "            \n",
    "            elif strategy == 'kekulize':\n",
    "                # Kekulization变体\n",
    "                try:\n",
    "                    Chem.Kekulize(mol)\n",
    "                    kek_smiles = Chem.MolToSmiles(mol, kekuleSmiles=True)\n",
    "                    augmented.add(sanitize_smiles(kek_smiles))\n",
    "                    \n",
    "                    # 添加不同的Kekulization形式\n",
    "                    for _ in range(min(5, n_augmentations)):\n",
    "                        new_mol = Chem.MolFromSmiles(kek_smiles)\n",
    "                        if new_mol:\n",
    "                            Chem.Kekulize(new_mol)\n",
    "                            new_smiles = Chem.MolToSmiles(new_mol, kekuleSmiles=True, doRandom=True)\n",
    "                            augmented.add(sanitize_smiles(new_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'stereo_enum':\n",
    "                # 立体化学枚举\n",
    "                try:\n",
    "                    # 移除立体化学\n",
    "                    Chem.RemoveStereochemistry(mol)\n",
    "                    no_stereo = Chem.MolToSmiles(mol)\n",
    "                    augmented.add(sanitize_smiles(no_stereo))\n",
    "                    \n",
    "                    # 添加随机立体中心\n",
    "                    for _ in range(n_augmentations // 2):\n",
    "                        new_mol = Chem.MolFromSmiles(no_stereo)\n",
    "                        if new_mol:\n",
    "                            Chem.AssignStereochemistry(new_mol, force=True, cleanIt=True)\n",
    "                            stereo_smiles = Chem.MolToSmiles(new_mol, isomericSmiles=True)\n",
    "                            augmented.add(sanitize_smiles(stereo_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'fragment':\n",
    "                # BRICS分解和重组\n",
    "                try:\n",
    "                    fragments = list(BRICS.BRICSDecompose(mol))\n",
    "                    if len(fragments) > 1:\n",
    "                        # 随机组合片段\n",
    "                        for _ in range(min(max_fragment_combinations, n_augmentations)):\n",
    "                            n_frags = random.randint(2, min(len(fragments), 3))\n",
    "                            selected_frags = random.sample(fragments, n_frags)\n",
    "                            new_mol = BRICS.BRICSBuild([Chem.MolFromSmiles(f) for f in selected_frags])\n",
    "                            if new_mol:\n",
    "                                new_smiles = Chem.MolToSmiles(new_mol)\n",
    "                                augmented.add(sanitize_smiles(new_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'rotate':\n",
    "                # 原子顺序旋转\n",
    "                try:\n",
    "                    for _ in range(n_augmentations // 2):\n",
    "                        rot_smiles = Chem.MolToSmiles(mol, \n",
    "                                                     doRandom=True,\n",
    "                                                     canonical=False,\n",
    "                                                     rootedAtAtom=random.randint(0, mol.GetNumAtoms()-1))\n",
    "                        augmented.add(sanitize_smiles(rot_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return list(augmented)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"警告: {strategy}处理{smiles}时出错: {e}\")\n",
    "            return [smiles]\n",
    "    \n",
    "    augmented_rows = []\n",
    "    stats: Dict[str, int] = {strategy: 0 for strategy in augmentation_strategies}\n",
    "    stats['original'] = 0\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"增强SMILES\"):\n",
    "        original_smiles = row[smiles_column]\n",
    "        \n",
    "        if preserve_original:\n",
    "            original_row = row.to_dict()\n",
    "            original_row['augmentation_strategy'] = 'original'\n",
    "            original_row['is_augmented'] = False\n",
    "            augmented_rows.append(original_row)\n",
    "            stats['original'] += 1\n",
    "        \n",
    "        for strategy in augmentation_strategies:\n",
    "            strategy_smiles = apply_augmentation_strategy(original_smiles, strategy)\n",
    "            \n",
    "            for aug_smiles in strategy_smiles:\n",
    "                if aug_smiles != original_smiles:\n",
    "                    new_row = row.to_dict().copy()\n",
    "                    new_row[smiles_column] = aug_smiles\n",
    "                    new_row['augmentation_strategy'] = strategy\n",
    "                    new_row['is_augmented'] = True\n",
    "                    augmented_rows.append(new_row)\n",
    "                    stats[strategy] += 1\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    augmented_df = augmented_df.reset_index(drop=True)\n",
    "    \n",
    "    # 打印详细统计信息\n",
    "    print(\"\\n增强结果统计:\")\n",
    "    print(f\"原始数据集大小: {len(df)}\")\n",
    "    print(f\"增强后数据集大小: {len(augmented_df)}\")\n",
    "    print(f\"总体增强倍数: {len(augmented_df) / len(df):.2f}x\")\n",
    "    print(\"\\n各策略贡献:\")\n",
    "    for strategy, count in stats.items():\n",
    "        print(f\"- {strategy}: {count}条记录 ({count/len(df):.2f}x)\")\n",
    "    \n",
    "    return augmented_df\n",
    "\n",
    "test = augment_smiles_dataset(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import BRICS\n",
    "import random\n",
    "from typing import Optional, List, Union, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "def augment_smiles_dataset(df: pd.DataFrame,\n",
    "                          smiles_column: str = 'SMILES',\n",
    "                          augmentation_strategies: List[str] = ['enumeration', 'kekulize', 'stereo_enum', 'fragment', 'rotate'],\n",
    "                          n_augmentations: int = 100,\n",
    "                          preserve_original: bool = True,\n",
    "                          random_seed: Optional[int] = None,\n",
    "                          max_fragment_combinations: int = 2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    增强的SMILES数据集生成器\n",
    "    \n",
    "    参数:\n",
    "    - df: 输入的DataFrame\n",
    "    - smiles_column: SMILES列的名称\n",
    "    - augmentation_strategies: 增强策略列表\n",
    "    - n_augmentations: 每个策略的增强次数\n",
    "    - preserve_original: 是否保留原始SMILES\n",
    "    - random_seed: 随机种子\n",
    "    - max_fragment_combinations: 片段重组的最大组合数\n",
    "    \n",
    "    返回:\n",
    "    - 增强后的DataFrame\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    def sanitize_smiles(smiles: str) -> str:\n",
    "        \"\"\"净化SMILES字符串\"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                return Chem.MolToSmiles(mol, canonical=True)\n",
    "        except:\n",
    "            pass\n",
    "        return smiles\n",
    "    \n",
    "    def apply_augmentation_strategy(smiles: str, strategy: str) -> List[str]:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                return [smiles]\n",
    "            \n",
    "            augmented = set()  # 使用集合避免重复\n",
    "            \n",
    "            if strategy == 'enumeration':\n",
    "                # 增强的SMILES枚举\n",
    "                for _ in range(n_augmentations):\n",
    "                    enum_smiles = Chem.MolToSmiles(mol, \n",
    "                                                 canonical=False, \n",
    "                                                 doRandom=True,\n",
    "                                                 isomericSmiles=True)\n",
    "                    augmented.add(sanitize_smiles(enum_smiles))\n",
    "            \n",
    "            elif strategy == 'kekulize':\n",
    "                # Kekulization变体\n",
    "                try:\n",
    "                    Chem.Kekulize(mol)\n",
    "                    kek_smiles = Chem.MolToSmiles(mol, kekuleSmiles=True)\n",
    "                    augmented.add(sanitize_smiles(kek_smiles))\n",
    "                    \n",
    "                    # 添加不同的Kekulization形式\n",
    "                    for _ in range(min(5, n_augmentations)):\n",
    "                        new_mol = Chem.MolFromSmiles(kek_smiles)\n",
    "                        if new_mol:\n",
    "                            Chem.Kekulize(new_mol)\n",
    "                            new_smiles = Chem.MolToSmiles(new_mol, kekuleSmiles=True, doRandom=True)\n",
    "                            augmented.add(sanitize_smiles(new_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'stereo_enum':\n",
    "                # 立体化学枚举\n",
    "                try:\n",
    "                    # 移除立体化学\n",
    "                    Chem.RemoveStereochemistry(mol)\n",
    "                    no_stereo = Chem.MolToSmiles(mol)\n",
    "                    augmented.add(sanitize_smiles(no_stereo))\n",
    "                    \n",
    "                    # 添加随机立体中心\n",
    "                    for _ in range(n_augmentations // 2):\n",
    "                        new_mol = Chem.MolFromSmiles(no_stereo)\n",
    "                        if new_mol:\n",
    "                            Chem.AssignStereochemistry(new_mol, force=True, cleanIt=True)\n",
    "                            stereo_smiles = Chem.MolToSmiles(new_mol, isomericSmiles=True)\n",
    "                            augmented.add(sanitize_smiles(stereo_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'fragment':\n",
    "                # BRICS分解和重组\n",
    "                try:\n",
    "                    fragments = list(BRICS.BRICSDecompose(mol))\n",
    "                    if len(fragments) > 1:\n",
    "                        # 随机组合片段\n",
    "                        for _ in range(min(max_fragment_combinations, n_augmentations)):\n",
    "                            n_frags = random.randint(2, min(len(fragments), 3))\n",
    "                            selected_frags = random.sample(fragments, n_frags)\n",
    "                            new_mol = BRICS.BRICSBuild([Chem.MolFromSmiles(f) for f in selected_frags])\n",
    "                            if new_mol:\n",
    "                                new_smiles = Chem.MolToSmiles(new_mol)\n",
    "                                augmented.add(sanitize_smiles(new_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'rotate':\n",
    "                # 原子顺序旋转\n",
    "                try:\n",
    "                    for _ in range(n_augmentations // 2):\n",
    "                        rot_smiles = Chem.MolToSmiles(mol, \n",
    "                                                     doRandom=True,\n",
    "                                                     canonical=False,\n",
    "                                                     rootedAtAtom=random.randint(0, mol.GetNumAtoms()-1))\n",
    "                        augmented.add(sanitize_smiles(rot_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return list(augmented)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"警告: {strategy}处理{smiles}时出错: {e}\")\n",
    "            return [smiles]\n",
    "    \n",
    "    augmented_rows = []\n",
    "    stats: Dict[str, int] = {strategy: 0 for strategy in augmentation_strategies}\n",
    "    stats['original'] = 0\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"增强SMILES\"):\n",
    "        original_smiles = row[smiles_column]\n",
    "        \n",
    "        if preserve_original:\n",
    "            original_row = row.to_dict()\n",
    "            original_row['augmentation_strategy'] = 'original'\n",
    "            original_row['is_augmented'] = False\n",
    "            augmented_rows.append(original_row)\n",
    "            stats['original'] += 1\n",
    "        \n",
    "        for strategy in augmentation_strategies:\n",
    "            strategy_smiles = apply_augmentation_strategy(original_smiles, strategy)\n",
    "            \n",
    "            for aug_smiles in strategy_smiles:\n",
    "                if aug_smiles != original_smiles:\n",
    "                    new_row = row.to_dict().copy()\n",
    "                    new_row[smiles_column] = aug_smiles\n",
    "                    new_row['augmentation_strategy'] = strategy\n",
    "                    new_row['is_augmented'] = True\n",
    "                    augmented_rows.append(new_row)\n",
    "                    stats[strategy] += 1\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    augmented_df = augmented_df.reset_index(drop=True)\n",
    "    \n",
    "    # 打印详细统计信息\n",
    "    print(\"\\n增强结果统计:\")\n",
    "    print(f\"原始数据集大小: {len(df)}\")\n",
    "    print(f\"增强后数据集大小: {len(augmented_df)}\")\n",
    "    print(f\"总体增强倍数: {len(augmented_df) / len(df):.2f}x\")\n",
    "    print(\"\\n各策略贡献:\")\n",
    "    for strategy, count in stats.items():\n",
    "        print(f\"- {strategy}: {count}条记录 ({count/len(df):.2f}x)\")\n",
    "    \n",
    "    return augmented_df\n",
    "\n",
    "test = augment_smiles_dataset(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import BRICS\n",
    "import random\n",
    "from typing import Optional, List, Union, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "def augment_smiles_dataset(df: pd.DataFrame,\n",
    "                          smiles_column: str = 'SMILES',\n",
    "                          augmentation_strategies: List[str] = ['enumeration', 'kekulize', 'stereo_enum', 'fragment', 'rotate'],\n",
    "                          n_augmentations: int = 100,\n",
    "                          preserve_original: bool = True,\n",
    "                          random_seed: Optional[int] = None,\n",
    "                          max_fragment_combinations: int = 2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    增强的SMILES数据集生成器\n",
    "    \n",
    "    参数:\n",
    "    - df: 输入的DataFrame\n",
    "    - smiles_column: SMILES列的名称\n",
    "    - augmentation_strategies: 增强策略列表\n",
    "    - n_augmentations: 每个策略的增强次数\n",
    "    - preserve_original: 是否保留原始SMILES\n",
    "    - random_seed: 随机种子\n",
    "    - max_fragment_combinations: 片段重组的最大组合数\n",
    "    \n",
    "    返回:\n",
    "    - 增强后的DataFrame\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    def sanitize_smiles(smiles: str) -> str:\n",
    "        \"\"\"净化SMILES字符串\"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                return Chem.MolToSmiles(mol, canonical=True)\n",
    "        except:\n",
    "            pass\n",
    "        return smiles\n",
    "    \n",
    "    def apply_augmentation_strategy(smiles: str, strategy: str) -> List[str]:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                return [smiles]\n",
    "            \n",
    "            augmented = set()  # 使用集合避免重复\n",
    "            \n",
    "            if strategy == 'enumeration':\n",
    "                # 增强的SMILES枚举\n",
    "                for _ in range(n_augmentations):\n",
    "                    enum_smiles = Chem.MolToSmiles(mol, \n",
    "                                                 canonical=False, \n",
    "                                                 doRandom=True,\n",
    "                                                 isomericSmiles=True)\n",
    "                    augmented.add(sanitize_smiles(enum_smiles))\n",
    "            \n",
    "            elif strategy == 'kekulize':\n",
    "                # Kekulization变体\n",
    "                try:\n",
    "                    Chem.Kekulize(mol)\n",
    "                    kek_smiles = Chem.MolToSmiles(mol, kekuleSmiles=True)\n",
    "                    augmented.add(sanitize_smiles(kek_smiles))\n",
    "                    \n",
    "                    # 添加不同的Kekulization形式\n",
    "                    for _ in range(min(5, n_augmentations)):\n",
    "                        new_mol = Chem.MolFromSmiles(kek_smiles)\n",
    "                        if new_mol:\n",
    "                            Chem.Kekulize(new_mol)\n",
    "                            new_smiles = Chem.MolToSmiles(new_mol, kekuleSmiles=True, doRandom=True)\n",
    "                            augmented.add(sanitize_smiles(new_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'stereo_enum':\n",
    "                # 立体化学枚举\n",
    "                try:\n",
    "                    # 移除立体化学\n",
    "                    Chem.RemoveStereochemistry(mol)\n",
    "                    no_stereo = Chem.MolToSmiles(mol)\n",
    "                    augmented.add(sanitize_smiles(no_stereo))\n",
    "                    \n",
    "                    # 添加随机立体中心\n",
    "                    for _ in range(n_augmentations // 2):\n",
    "                        new_mol = Chem.MolFromSmiles(no_stereo)\n",
    "                        if new_mol:\n",
    "                            Chem.AssignStereochemistry(new_mol, force=True, cleanIt=True)\n",
    "                            stereo_smiles = Chem.MolToSmiles(new_mol, isomericSmiles=True)\n",
    "                            augmented.add(sanitize_smiles(stereo_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'fragment':\n",
    "                # BRICS分解和重组\n",
    "                try:\n",
    "                    fragments = list(BRICS.BRICSDecompose(mol))\n",
    "                    if len(fragments) > 1:\n",
    "                        # 随机组合片段\n",
    "                        for _ in range(min(max_fragment_combinations, n_augmentations)):\n",
    "                            n_frags = random.randint(2, min(len(fragments), 3))\n",
    "                            selected_frags = random.sample(fragments, n_frags)\n",
    "                            new_mol = BRICS.BRICSBuild([Chem.MolFromSmiles(f) for f in selected_frags])\n",
    "                            if new_mol:\n",
    "                                new_smiles = Chem.MolToSmiles(new_mol)\n",
    "                                augmented.add(sanitize_smiles(new_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'rotate':\n",
    "                # 原子顺序旋转\n",
    "                try:\n",
    "                    for _ in range(n_augmentations // 2):\n",
    "                        rot_smiles = Chem.MolToSmiles(mol, \n",
    "                                                     doRandom=True,\n",
    "                                                     canonical=False,\n",
    "                                                     rootedAtAtom=random.randint(0, mol.GetNumAtoms()-1))\n",
    "                        augmented.add(sanitize_smiles(rot_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return list(augmented)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"警告: {strategy}处理{smiles}时出错: {e}\")\n",
    "            return [smiles]\n",
    "    \n",
    "    augmented_rows = []\n",
    "    stats: Dict[str, int] = {strategy: 0 for strategy in augmentation_strategies}\n",
    "    stats['original'] = 0\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"增强SMILES\"):\n",
    "        original_smiles = row[smiles_column]\n",
    "        \n",
    "        if preserve_original:\n",
    "            original_row = row.to_dict()\n",
    "            original_row['augmentation_strategy'] = 'original'\n",
    "            original_row['is_augmented'] = False\n",
    "            augmented_rows.append(original_row)\n",
    "            stats['original'] += 1\n",
    "        \n",
    "        for strategy in augmentation_strategies:\n",
    "            strategy_smiles = apply_augmentation_strategy(original_smiles, strategy)\n",
    "            \n",
    "            for aug_smiles in strategy_smiles:\n",
    "                if aug_smiles != original_smiles:\n",
    "                    new_row = row.to_dict().copy()\n",
    "                    new_row[smiles_column] = aug_smiles\n",
    "                    new_row['augmentation_strategy'] = strategy\n",
    "                    new_row['is_augmented'] = True\n",
    "                    augmented_rows.append(new_row)\n",
    "                    stats[strategy] += 1\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    augmented_df = augmented_df.reset_index(drop=True)\n",
    "    \n",
    "    # 打印详细统计信息\n",
    "    print(\"\\n增强结果统计:\")\n",
    "    print(f\"原始数据集大小: {len(df)}\")\n",
    "    print(f\"增强后数据集大小: {len(augmented_df)}\")\n",
    "    print(f\"总体增强倍数: {len(augmented_df) / len(df):.2f}x\")\n",
    "    print(\"\\n各策略贡献:\")\n",
    "    for strategy, count in stats.items():\n",
    "        print(f\"- {strategy}: {count}条记录 ({count/len(df):.2f}x)\")\n",
    "    \n",
    "    return augmented_df\n",
    "\n",
    "test = augment_smiles_dataset(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (11.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit==2025.3.3) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit==2025.3.3) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit==2025.3.3) (2024.2.0)\n",
      "rdkit is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from transformers import PreTrainedModel, AutoConfig, BertModel, BertTokenizerFast, BertConfig, AutoModel, AutoTokenizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torch import nn\n",
    "from transformers.activations import ACT2FN\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class ContextPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        pooler_size = getattr(config, 'pooler_hidden_size', config.hidden_size)\n",
    "        self.dense = nn.Linear(pooler_size, pooler_size)\n",
    "        \n",
    "        dropout_prob = getattr(config, 'pooler_dropout', config.hidden_dropout_prob)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.activation = getattr(config, 'pooler_hidden_act', config.hidden_act)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        context_token = hidden_states[:, 0] # CLS token\n",
    "        context_token = self.dropout(context_token)\n",
    "        pooled_output = self.dense(context_token)\n",
    "        pooled_output = ACT2FN[self.activation](pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class CustomModel(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.backbone = AutoModel.from_config(config)\n",
    "        \n",
    "        self.pooler = ContextPooler(config)\n",
    "\n",
    "        pooler_output_dim = getattr(config, 'pooler_hidden_size', config.hidden_size)\n",
    "        self.output = torch.nn.Linear(pooler_output_dim, 1) # Still predicting one label at a time. Kinda stupid\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        scaler,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.backbone(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "\n",
    "        pooled_output = self.pooler(outputs.last_hidden_state)\n",
    "        \n",
    "        # Final regression output\n",
    "        regression_output = self.output(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        true_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "            unscaled_labels = scaler.inverse_transform(labels.cpu().numpy())\n",
    "            unscaled_outputs = scaler.inverse_transform(regression_output.cpu().detach().numpy())\n",
    "            \n",
    "            loss = loss_fn(regression_output, labels)\n",
    "            true_loss = mean_absolute_error(unscaled_outputs, unscaled_labels)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": regression_output,\n",
    "            \"true_loss\": true_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[0;32m/tmp/ipykernel_35/2779921138.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maugmented_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 87\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_smiles_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "def tokenize_smiles(seq):\n",
    "    seq = [tokenizer.cls_token + smiles for smiles in seq] # If we pass a string, tokenizer will smartly think we want to create a sequence for each symbol\n",
    "    tokenized = tokenizer(seq, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "    return tokenized\n",
    "\n",
    "def load_model(path):\n",
    "    config = AutoConfig.from_pretrained('/kaggle/input/smiles-deberta77m-tokenizer')\n",
    "    model = CustomModel(config).cuda()\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_predictions(model, scaler, smiles_seq):\n",
    "    aggregated_preds = []\n",
    "    for smiles in smiles_seq:\n",
    "        smiles = [smiles]\n",
    "        smiles_tokenized = tokenize_smiles(smiles)\n",
    "\n",
    "        input_ids = smiles_tokenized['input_ids'].cuda()\n",
    "        attention_mask = smiles_tokenized['attention_mask'].cuda()\n",
    "        with torch.no_grad():\n",
    "            preds = model(input_ids=input_ids, scaler=scaler, attention_mask=attention_mask)['logits'].cpu().numpy()\n",
    "        \n",
    "        true_preds = scaler.inverse_transform(preds).flatten()\n",
    "        aggregated_preds.append(true_preds.tolist())\n",
    "    return np.array(aggregated_preds)\n",
    "\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')\n",
    "test_copy = test.copy()\n",
    "\n",
    "smiles_test = test['SMILES'].to_list()\n",
    "\n",
    "targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "scalers = joblib.load('/kaggle/input/smiles-bert-models/target_scalers.pkl')\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/smiles-deberta77m-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import random\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "def augment_smiles_dataset(df: pd.DataFrame,\n",
    "                               smiles_column: str = 'SMILES',\n",
    "                               augmentation_strategies: List[str] = ['enumeration', 'kekulize', 'stereo_enum'],\n",
    "                               n_augmentations: int = 100,\n",
    "                               preserve_original: bool = True,\n",
    "                               random_seed: Optional[int] = None) -> pd.DataFrame:\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    def apply_augmentation_strategy(smiles: str, strategy: str) -> List[str]:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                return [smiles]\n",
    "            \n",
    "            augmented = []\n",
    "            \n",
    "            if strategy == 'enumeration':\n",
    "                # Standard SMILES enumeration\n",
    "                for _ in range(n_augmentations):\n",
    "                    enum_smiles = Chem.MolToSmiles(mol, \n",
    "                                                 canonical=False, \n",
    "                                                 doRandom=True,\n",
    "                                                 isomericSmiles=True)\n",
    "                    augmented.append(enum_smiles)\n",
    "            \n",
    "            elif strategy == 'kekulize':\n",
    "                # Kekulization variants\n",
    "                try:\n",
    "                    Chem.Kekulize(mol)\n",
    "                    kek_smiles = Chem.MolToSmiles(mol, kekuleSmiles=True)\n",
    "                    augmented.append(kek_smiles)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'stereo_enum':\n",
    "                # Stereochemistry enumeration\n",
    "                for _ in range(n_augmentations // 2):\n",
    "                    # Remove stereochemistry\n",
    "                    Chem.RemoveStereochemistry(mol)\n",
    "                    no_stereo = Chem.MolToSmiles(mol)\n",
    "                    augmented.append(no_stereo)\n",
    "            \n",
    "            return list(set(augmented))  # Remove duplicates\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in {strategy} for {smiles}: {e}\")\n",
    "            return [smiles]\n",
    "    \n",
    "    augmented_rows = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        original_smiles = row[smiles_column]\n",
    "        \n",
    "        if preserve_original:\n",
    "            original_row = row.to_dict()\n",
    "            original_row['augmentation_strategy'] = 'original'\n",
    "            original_row['is_augmented'] = False\n",
    "            augmented_rows.append(original_row)\n",
    "        \n",
    "        for strategy in augmentation_strategies:\n",
    "            strategy_smiles = apply_augmentation_strategy(original_smiles, strategy)\n",
    "            \n",
    "            for aug_smiles in strategy_smiles:\n",
    "                if aug_smiles != original_smiles:\n",
    "                    new_row = row.to_dict().copy()\n",
    "                    new_row[smiles_column] = aug_smiles\n",
    "                    new_row['augmentation_strategy'] = strategy\n",
    "                    new_row['is_augmented'] = True\n",
    "                    augmented_rows.append(new_row)\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    augmented_df = augmented_df.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Original size: {len(df)}, Augmented size: {len(augmented_df)}\")\n",
    "    print(f\"Augmentation factor: {len(augmented_df) / len(df):.2f}x\")\n",
    "    \n",
    "    return augmented_df\n",
    "\n",
    "test = augment_smiles_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mapping = {}\n",
    "\n",
    "for i in tqdm(range(len(targets))):\n",
    "    target = targets[i]\n",
    "    scaler = scalers[i]\n",
    "\n",
    "    model_path = f'/kaggle/input/smiles-bert-models/trained_smiles_model_{target}_target.pth' # Very sophisticated staff\n",
    "    model = load_model(model_path)\n",
    "    true_preds = []\n",
    "\n",
    "    for i, data in test.groupby('id'):\n",
    "        test_smiles = data['SMILES'].to_list()\n",
    "        augmented_preds = make_predictions(model, scaler, test_smiles)\n",
    "    \n",
    "        average_pred = np.median(augmented_preds)\n",
    "    \n",
    "        true_preds.append(float(average_pred.flatten()[0]))\n",
    "\n",
    "    preds_mapping[target] = true_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(preds_mapping)\n",
    "submission['id'] = test_copy['id']\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training notebook: [here](https://www.kaggle.com/code/defdet/polymer-bert-train?scriptVersionId=246123151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (11.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit==2025.3.3) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit==2025.3.3) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit==2025.3.3) (2024.2.0)\n",
      "rdkit is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from transformers import PreTrainedModel, AutoConfig, BertModel, BertTokenizerFast, BertConfig, AutoModel, AutoTokenizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torch import nn\n",
    "from transformers.activations import ACT2FN\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class ContextPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        pooler_size = getattr(config, 'pooler_hidden_size', config.hidden_size)\n",
    "        self.dense = nn.Linear(pooler_size, pooler_size)\n",
    "        \n",
    "        dropout_prob = getattr(config, 'pooler_dropout', config.hidden_dropout_prob)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.activation = getattr(config, 'pooler_hidden_act', config.hidden_act)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        context_token = hidden_states[:, 0] # CLS token\n",
    "        context_token = self.dropout(context_token)\n",
    "        pooled_output = self.dense(context_token)\n",
    "        pooled_output = ACT2FN[self.activation](pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class CustomModel(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.backbone = AutoModel.from_config(config)\n",
    "        \n",
    "        self.pooler = ContextPooler(config)\n",
    "\n",
    "        pooler_output_dim = getattr(config, 'pooler_hidden_size', config.hidden_size)\n",
    "        self.output = torch.nn.Linear(pooler_output_dim, 1) # Still predicting one label at a time. Kinda stupid\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        scaler,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.backbone(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "\n",
    "        pooled_output = self.pooler(outputs.last_hidden_state)\n",
    "        \n",
    "        # Final regression output\n",
    "        regression_output = self.output(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        true_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "            unscaled_labels = scaler.inverse_transform(labels.cpu().numpy())\n",
    "            unscaled_outputs = scaler.inverse_transform(regression_output.cpu().detach().numpy())\n",
    "            \n",
    "            loss = loss_fn(regression_output, labels)\n",
    "            true_loss = mean_absolute_error(unscaled_outputs, unscaled_labels)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": regression_output,\n",
    "            \"true_loss\": true_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[0;32m/tmp/ipykernel_35/2779921138.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maugmented_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 87\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_smiles_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "def tokenize_smiles(seq):\n",
    "    seq = [tokenizer.cls_token + smiles for smiles in seq] # If we pass a string, tokenizer will smartly think we want to create a sequence for each symbol\n",
    "    tokenized = tokenizer(seq, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "    return tokenized\n",
    "\n",
    "def load_model(path):\n",
    "    config = AutoConfig.from_pretrained('/kaggle/input/smiles-deberta77m-tokenizer')\n",
    "    model = CustomModel(config).cuda()\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_predictions(model, scaler, smiles_seq):\n",
    "    aggregated_preds = []\n",
    "    for smiles in smiles_seq:\n",
    "        smiles = [smiles]\n",
    "        smiles_tokenized = tokenize_smiles(smiles)\n",
    "\n",
    "        input_ids = smiles_tokenized['input_ids'].cuda()\n",
    "        attention_mask = smiles_tokenized['attention_mask'].cuda()\n",
    "        with torch.no_grad():\n",
    "            preds = model(input_ids=input_ids, scaler=scaler, attention_mask=attention_mask)['logits'].cpu().numpy()\n",
    "        \n",
    "        true_preds = scaler.inverse_transform(preds).flatten()\n",
    "        aggregated_preds.append(true_preds.tolist())\n",
    "    return np.array(aggregated_preds)\n",
    "\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')\n",
    "test_copy = test.copy()\n",
    "\n",
    "smiles_test = test['SMILES'].to_list()\n",
    "\n",
    "targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "scalers = joblib.load('/kaggle/input/smiles-bert-models/target_scalers.pkl')\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/smiles-deberta77m-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import random\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "def augment_smiles_dataset(df: pd.DataFrame,\n",
    "                               smiles_column: str = 'SMILES',\n",
    "                               augmentation_strategies: List[str] = ['enumeration', 'kekulize', 'stereo_enum'],\n",
    "                               n_augmentations: int = 100,\n",
    "                               preserve_original: bool = True,\n",
    "                               random_seed: Optional[int] = None) -> pd.DataFrame:\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    def apply_augmentation_strategy(smiles: str, strategy: str) -> List[str]:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                return [smiles]\n",
    "            \n",
    "            augmented = []\n",
    "            \n",
    "            if strategy == 'enumeration':\n",
    "                # Standard SMILES enumeration\n",
    "                for _ in range(n_augmentations):\n",
    "                    enum_smiles = Chem.MolToSmiles(mol, \n",
    "                                                 canonical=False, \n",
    "                                                 doRandom=True,\n",
    "                                                 isomericSmiles=True)\n",
    "                    augmented.append(enum_smiles)\n",
    "            \n",
    "            elif strategy == 'kekulize':\n",
    "                # Kekulization variants\n",
    "                try:\n",
    "                    Chem.Kekulize(mol)\n",
    "                    kek_smiles = Chem.MolToSmiles(mol, kekuleSmiles=True)\n",
    "                    augmented.append(kek_smiles)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'stereo_enum':\n",
    "                # Stereochemistry enumeration\n",
    "                for _ in range(n_augmentations // 2):\n",
    "                    # Remove stereochemistry\n",
    "                    Chem.RemoveStereochemistry(mol)\n",
    "                    no_stereo = Chem.MolToSmiles(mol)\n",
    "                    augmented.append(no_stereo)\n",
    "            \n",
    "            return list(set(augmented))  # Remove duplicates\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in {strategy} for {smiles}: {e}\")\n",
    "            return [smiles]\n",
    "    \n",
    "    augmented_rows = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        original_smiles = row[smiles_column]\n",
    "        \n",
    "        if preserve_original:\n",
    "            original_row = row.to_dict()\n",
    "            original_row['augmentation_strategy'] = 'original'\n",
    "            original_row['is_augmented'] = False\n",
    "            augmented_rows.append(original_row)\n",
    "        \n",
    "        for strategy in augmentation_strategies:\n",
    "            strategy_smiles = apply_augmentation_strategy(original_smiles, strategy)\n",
    "            \n",
    "            for aug_smiles in strategy_smiles:\n",
    "                if aug_smiles != original_smiles:\n",
    "                    new_row = row.to_dict().copy()\n",
    "                    new_row[smiles_column] = aug_smiles\n",
    "                    new_row['augmentation_strategy'] = strategy\n",
    "                    new_row['is_augmented'] = True\n",
    "                    augmented_rows.append(new_row)\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    augmented_df = augmented_df.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Original size: {len(df)}, Augmented size: {len(augmented_df)}\")\n",
    "    print(f\"Augmentation factor: {len(augmented_df) / len(df):.2f}x\")\n",
    "    \n",
    "    return augmented_df\n",
    "\n",
    "test = augment_smiles_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mapping = {}\n",
    "\n",
    "for i in tqdm(range(len(targets))):\n",
    "    target = targets[i]\n",
    "    scaler = scalers[i]\n",
    "\n",
    "    model_path = f'/kaggle/input/smiles-bert-models/trained_smiles_model_{target}_target.pth' # Very sophisticated staff\n",
    "    model = load_model(model_path)\n",
    "    true_preds = []\n",
    "\n",
    "    for i, data in test.groupby('id'):\n",
    "        test_smiles = data['SMILES'].to_list()\n",
    "        augmented_preds = make_predictions(model, scaler, test_smiles)\n",
    "    \n",
    "        average_pred = np.median(augmented_preds)\n",
    "    \n",
    "        true_preds.append(float(average_pred.flatten()[0]))\n",
    "\n",
    "    preds_mapping[target] = true_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(preds_mapping)\n",
    "submission['id'] = test_copy['id']\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training notebook: [here](https://www.kaggle.com/code/defdet/polymer-bert-train?scriptVersionId=246123151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (11.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit==2025.3.3) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit==2025.3.3) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit==2025.3.3) (2024.2.0)\n",
      "rdkit is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from transformers import PreTrainedModel, AutoConfig, BertModel, BertTokenizerFast, BertConfig, AutoModel, AutoTokenizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torch import nn\n",
    "from transformers.activations import ACT2FN\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class ContextPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        pooler_size = getattr(config, 'pooler_hidden_size', config.hidden_size)\n",
    "        self.dense = nn.Linear(pooler_size, pooler_size)\n",
    "        \n",
    "        dropout_prob = getattr(config, 'pooler_dropout', config.hidden_dropout_prob)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.activation = getattr(config, 'pooler_hidden_act', config.hidden_act)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        context_token = hidden_states[:, 0] # CLS token\n",
    "        context_token = self.dropout(context_token)\n",
    "        pooled_output = self.dense(context_token)\n",
    "        pooled_output = ACT2FN[self.activation](pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class CustomModel(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.backbone = AutoModel.from_config(config)\n",
    "        \n",
    "        self.pooler = ContextPooler(config)\n",
    "\n",
    "        pooler_output_dim = getattr(config, 'pooler_hidden_size', config.hidden_size)\n",
    "        self.output = torch.nn.Linear(pooler_output_dim, 1) # Still predicting one label at a time. Kinda stupid\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        scaler,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.backbone(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "\n",
    "        pooled_output = self.pooler(outputs.last_hidden_state)\n",
    "        \n",
    "        # Final regression output\n",
    "        regression_output = self.output(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        true_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "            unscaled_labels = scaler.inverse_transform(labels.cpu().numpy())\n",
    "            unscaled_outputs = scaler.inverse_transform(regression_output.cpu().detach().numpy())\n",
    "            \n",
    "            loss = loss_fn(regression_output, labels)\n",
    "            true_loss = mean_absolute_error(unscaled_outputs, unscaled_labels)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": regression_output,\n",
    "            \"true_loss\": true_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[0;32m/tmp/ipykernel_35/2779921138.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maugmented_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 87\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_smiles_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "def tokenize_smiles(seq):\n",
    "    seq = [tokenizer.cls_token + smiles for smiles in seq] # If we pass a string, tokenizer will smartly think we want to create a sequence for each symbol\n",
    "    tokenized = tokenizer(seq, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "    return tokenized\n",
    "\n",
    "def load_model(path):\n",
    "    config = AutoConfig.from_pretrained('/kaggle/input/smiles-deberta77m-tokenizer')\n",
    "    model = CustomModel(config).cuda()\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_predictions(model, scaler, smiles_seq):\n",
    "    aggregated_preds = []\n",
    "    for smiles in smiles_seq:\n",
    "        smiles = [smiles]\n",
    "        smiles_tokenized = tokenize_smiles(smiles)\n",
    "\n",
    "        input_ids = smiles_tokenized['input_ids'].cuda()\n",
    "        attention_mask = smiles_tokenized['attention_mask'].cuda()\n",
    "        with torch.no_grad():\n",
    "            preds = model(input_ids=input_ids, scaler=scaler, attention_mask=attention_mask)['logits'].cpu().numpy()\n",
    "        \n",
    "        true_preds = scaler.inverse_transform(preds).flatten()\n",
    "        aggregated_preds.append(true_preds.tolist())\n",
    "    return np.array(aggregated_preds)\n",
    "\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')\n",
    "test_copy = test.copy()\n",
    "\n",
    "smiles_test = test['SMILES'].to_list()\n",
    "\n",
    "targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "scalers = joblib.load('/kaggle/input/smiles-bert-models/target_scalers.pkl')\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/smiles-deberta77m-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import random\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "def augment_smiles_dataset(df: pd.DataFrame,\n",
    "                               smiles_column: str = 'SMILES',\n",
    "                               augmentation_strategies: List[str] = ['enumeration', 'kekulize', 'stereo_enum'],\n",
    "                               n_augmentations: int = 100,\n",
    "                               preserve_original: bool = True,\n",
    "                               random_seed: Optional[int] = None) -> pd.DataFrame:\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    def apply_augmentation_strategy(smiles: str, strategy: str) -> List[str]:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                return [smiles]\n",
    "            \n",
    "            augmented = []\n",
    "            \n",
    "            if strategy == 'enumeration':\n",
    "                # Standard SMILES enumeration\n",
    "                for _ in range(n_augmentations):\n",
    "                    enum_smiles = Chem.MolToSmiles(mol, \n",
    "                                                 canonical=False, \n",
    "                                                 doRandom=True,\n",
    "                                                 isomericSmiles=True)\n",
    "                    augmented.append(enum_smiles)\n",
    "            \n",
    "            elif strategy == 'kekulize':\n",
    "                # Kekulization variants\n",
    "                try:\n",
    "                    Chem.Kekulize(mol)\n",
    "                    kek_smiles = Chem.MolToSmiles(mol, kekuleSmiles=True)\n",
    "                    augmented.append(kek_smiles)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'stereo_enum':\n",
    "                # Stereochemistry enumeration\n",
    "                for _ in range(n_augmentations // 2):\n",
    "                    # Remove stereochemistry\n",
    "                    Chem.RemoveStereochemistry(mol)\n",
    "                    no_stereo = Chem.MolToSmiles(mol)\n",
    "                    augmented.append(no_stereo)\n",
    "            \n",
    "            return list(set(augmented))  # Remove duplicates\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in {strategy} for {smiles}: {e}\")\n",
    "            return [smiles]\n",
    "    \n",
    "    augmented_rows = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        original_smiles = row[smiles_column]\n",
    "        \n",
    "        if preserve_original:\n",
    "            original_row = row.to_dict()\n",
    "            original_row['augmentation_strategy'] = 'original'\n",
    "            original_row['is_augmented'] = False\n",
    "            augmented_rows.append(original_row)\n",
    "        \n",
    "        for strategy in augmentation_strategies:\n",
    "            strategy_smiles = apply_augmentation_strategy(original_smiles, strategy)\n",
    "            \n",
    "            for aug_smiles in strategy_smiles:\n",
    "                if aug_smiles != original_smiles:\n",
    "                    new_row = row.to_dict().copy()\n",
    "                    new_row[smiles_column] = aug_smiles\n",
    "                    new_row['augmentation_strategy'] = strategy\n",
    "                    new_row['is_augmented'] = True\n",
    "                    augmented_rows.append(new_row)\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    augmented_df = augmented_df.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Original size: {len(df)}, Augmented size: {len(augmented_df)}\")\n",
    "    print(f\"Augmentation factor: {len(augmented_df) / len(df):.2f}x\")\n",
    "    \n",
    "    return augmented_df\n",
    "\n",
    "test = augment_smiles_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mapping = {}\n",
    "\n",
    "for i in tqdm(range(len(targets))):\n",
    "    target = targets[i]\n",
    "    scaler = scalers[i]\n",
    "\n",
    "    model_path = f'/kaggle/input/smiles-bert-models/trained_smiles_model_{target}_target.pth' # Very sophisticated staff\n",
    "    model = load_model(model_path)\n",
    "    true_preds = []\n",
    "\n",
    "    for i, data in test.groupby('id'):\n",
    "        test_smiles = data['SMILES'].to_list()\n",
    "        augmented_preds = make_predictions(model, scaler, test_smiles)\n",
    "    \n",
    "        average_pred = np.median(augmented_preds)\n",
    "    \n",
    "        true_preds.append(float(average_pred.flatten()[0]))\n",
    "\n",
    "    preds_mapping[target] = true_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(preds_mapping)\n",
    "submission['id'] = test_copy['id']\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training notebook: [here](https://www.kaggle.com/code/defdet/polymer-bert-train?scriptVersionId=246123151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (11.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit==2025.3.3) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit==2025.3.3) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit==2025.3.3) (2024.2.0)\n",
      "rdkit is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from transformers import PreTrainedModel, AutoConfig, BertModel, BertTokenizerFast, BertConfig, AutoModel, AutoTokenizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torch import nn\n",
    "from transformers.activations import ACT2FN\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class ContextPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        pooler_size = getattr(config, 'pooler_hidden_size', config.hidden_size)\n",
    "        self.dense = nn.Linear(pooler_size, pooler_size)\n",
    "        \n",
    "        dropout_prob = getattr(config, 'pooler_dropout', config.hidden_dropout_prob)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.activation = getattr(config, 'pooler_hidden_act', config.hidden_act)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        context_token = hidden_states[:, 0] # CLS token\n",
    "        context_token = self.dropout(context_token)\n",
    "        pooled_output = self.dense(context_token)\n",
    "        pooled_output = ACT2FN[self.activation](pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class CustomModel(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.backbone = AutoModel.from_config(config)\n",
    "        \n",
    "        self.pooler = ContextPooler(config)\n",
    "\n",
    "        pooler_output_dim = getattr(config, 'pooler_hidden_size', config.hidden_size)\n",
    "        self.output = torch.nn.Linear(pooler_output_dim, 1) # Still predicting one label at a time. Kinda stupid\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        scaler,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.backbone(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "\n",
    "        pooled_output = self.pooler(outputs.last_hidden_state)\n",
    "        \n",
    "        # Final regression output\n",
    "        regression_output = self.output(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        true_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "            unscaled_labels = scaler.inverse_transform(labels.cpu().numpy())\n",
    "            unscaled_outputs = scaler.inverse_transform(regression_output.cpu().detach().numpy())\n",
    "            \n",
    "            loss = loss_fn(regression_output, labels)\n",
    "            true_loss = mean_absolute_error(unscaled_outputs, unscaled_labels)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": regression_output,\n",
    "            \"true_loss\": true_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[0;32m/tmp/ipykernel_35/2779921138.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maugmented_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 87\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_smiles_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "def tokenize_smiles(seq):\n",
    "    seq = [tokenizer.cls_token + smiles for smiles in seq] # If we pass a string, tokenizer will smartly think we want to create a sequence for each symbol\n",
    "    tokenized = tokenizer(seq, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "    return tokenized\n",
    "\n",
    "def load_model(path):\n",
    "    config = AutoConfig.from_pretrained('/kaggle/input/smiles-deberta77m-tokenizer')\n",
    "    model = CustomModel(config).cuda()\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_predictions(model, scaler, smiles_seq):\n",
    "    aggregated_preds = []\n",
    "    for smiles in smiles_seq:\n",
    "        smiles = [smiles]\n",
    "        smiles_tokenized = tokenize_smiles(smiles)\n",
    "\n",
    "        input_ids = smiles_tokenized['input_ids'].cuda()\n",
    "        attention_mask = smiles_tokenized['attention_mask'].cuda()\n",
    "        with torch.no_grad():\n",
    "            preds = model(input_ids=input_ids, scaler=scaler, attention_mask=attention_mask)['logits'].cpu().numpy()\n",
    "        \n",
    "        true_preds = scaler.inverse_transform(preds).flatten()\n",
    "        aggregated_preds.append(true_preds.tolist())\n",
    "    return np.array(aggregated_preds)\n",
    "\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')\n",
    "test_copy = test.copy()\n",
    "\n",
    "smiles_test = test['SMILES'].to_list()\n",
    "\n",
    "targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "scalers = joblib.load('/kaggle/input/smiles-bert-models/target_scalers.pkl')\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/smiles-deberta77m-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import random\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "def augment_smiles_dataset(df: pd.DataFrame,\n",
    "                               smiles_column: str = 'SMILES',\n",
    "                               augmentation_strategies: List[str] = ['enumeration', 'kekulize', 'stereo_enum'],\n",
    "                               n_augmentations: int = 100,\n",
    "                               preserve_original: bool = True,\n",
    "                               random_seed: Optional[int] = None) -> pd.DataFrame:\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    def apply_augmentation_strategy(smiles: str, strategy: str) -> List[str]:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                return [smiles]\n",
    "            \n",
    "            augmented = []\n",
    "            \n",
    "            if strategy == 'enumeration':\n",
    "                # Standard SMILES enumeration\n",
    "                for _ in range(n_augmentations):\n",
    "                    enum_smiles = Chem.MolToSmiles(mol, \n",
    "                                                 canonical=False, \n",
    "                                                 doRandom=True,\n",
    "                                                 isomericSmiles=True)\n",
    "                    augmented.append(enum_smiles)\n",
    "            \n",
    "            elif strategy == 'kekulize':\n",
    "                # Kekulization variants\n",
    "                try:\n",
    "                    Chem.Kekulize(mol)\n",
    "                    kek_smiles = Chem.MolToSmiles(mol, kekuleSmiles=True)\n",
    "                    augmented.append(kek_smiles)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'stereo_enum':\n",
    "                # Stereochemistry enumeration\n",
    "                for _ in range(n_augmentations // 2):\n",
    "                    # Remove stereochemistry\n",
    "                    Chem.RemoveStereochemistry(mol)\n",
    "                    no_stereo = Chem.MolToSmiles(mol)\n",
    "                    augmented.append(no_stereo)\n",
    "            \n",
    "            return list(set(augmented))  # Remove duplicates\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in {strategy} for {smiles}: {e}\")\n",
    "            return [smiles]\n",
    "    \n",
    "    augmented_rows = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        original_smiles = row[smiles_column]\n",
    "        \n",
    "        if preserve_original:\n",
    "            original_row = row.to_dict()\n",
    "            original_row['augmentation_strategy'] = 'original'\n",
    "            original_row['is_augmented'] = False\n",
    "            augmented_rows.append(original_row)\n",
    "        \n",
    "        for strategy in augmentation_strategies:\n",
    "            strategy_smiles = apply_augmentation_strategy(original_smiles, strategy)\n",
    "            \n",
    "            for aug_smiles in strategy_smiles:\n",
    "                if aug_smiles != original_smiles:\n",
    "                    new_row = row.to_dict().copy()\n",
    "                    new_row[smiles_column] = aug_smiles\n",
    "                    new_row['augmentation_strategy'] = strategy\n",
    "                    new_row['is_augmented'] = True\n",
    "                    augmented_rows.append(new_row)\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    augmented_df = augmented_df.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Original size: {len(df)}, Augmented size: {len(augmented_df)}\")\n",
    "    print(f\"Augmentation factor: {len(augmented_df) / len(df):.2f}x\")\n",
    "    \n",
    "    return augmented_df\n",
    "\n",
    "test = augment_smiles_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mapping = {}\n",
    "\n",
    "for i in tqdm(range(len(targets))):\n",
    "    target = targets[i]\n",
    "    scaler = scalers[i]\n",
    "\n",
    "    model_path = f'/kaggle/input/smiles-bert-models/trained_smiles_model_{target}_target.pth' # Very sophisticated staff\n",
    "    model = load_model(model_path)\n",
    "    true_preds = []\n",
    "\n",
    "    for i, data in test.groupby('id'):\n",
    "        test_smiles = data['SMILES'].to_list()\n",
    "        augmented_preds = make_predictions(model, scaler, test_smiles)\n",
    "    \n",
    "        average_pred = np.median(augmented_preds)\n",
    "    \n",
    "        true_preds.append(float(average_pred.flatten()[0]))\n",
    "\n",
    "    preds_mapping[target] = true_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(preds_mapping)\n",
    "submission['id'] = test_copy['id']\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training notebook: [here](https://www.kaggle.com/code/defdet/polymer-bert-train?scriptVersionId=246123151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (11.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit==2025.3.3) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit==2025.3.3) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit==2025.3.3) (2024.2.0)\n",
      "rdkit is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from transformers import PreTrainedModel, AutoConfig, BertModel, BertTokenizerFast, BertConfig, AutoModel, AutoTokenizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torch import nn\n",
    "from transformers.activations import ACT2FN\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class ContextPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        pooler_size = getattr(config, 'pooler_hidden_size', config.hidden_size)\n",
    "        self.dense = nn.Linear(pooler_size, pooler_size)\n",
    "        \n",
    "        dropout_prob = getattr(config, 'pooler_dropout', config.hidden_dropout_prob)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.activation = getattr(config, 'pooler_hidden_act', config.hidden_act)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        context_token = hidden_states[:, 0] # CLS token\n",
    "        context_token = self.dropout(context_token)\n",
    "        pooled_output = self.dense(context_token)\n",
    "        pooled_output = ACT2FN[self.activation](pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class CustomModel(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.backbone = AutoModel.from_config(config)\n",
    "        \n",
    "        self.pooler = ContextPooler(config)\n",
    "\n",
    "        pooler_output_dim = getattr(config, 'pooler_hidden_size', config.hidden_size)\n",
    "        self.output = torch.nn.Linear(pooler_output_dim, 1) # Still predicting one label at a time. Kinda stupid\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        scaler,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.backbone(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "\n",
    "        pooled_output = self.pooler(outputs.last_hidden_state)\n",
    "        \n",
    "        # Final regression output\n",
    "        regression_output = self.output(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        true_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "            unscaled_labels = scaler.inverse_transform(labels.cpu().numpy())\n",
    "            unscaled_outputs = scaler.inverse_transform(regression_output.cpu().detach().numpy())\n",
    "            \n",
    "            loss = loss_fn(regression_output, labels)\n",
    "            true_loss = mean_absolute_error(unscaled_outputs, unscaled_labels)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": regression_output,\n",
    "            \"true_loss\": true_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[0;32m/tmp/ipykernel_35/2779921138.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maugmented_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 87\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_smiles_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "def tokenize_smiles(seq):\n",
    "    seq = [tokenizer.cls_token + smiles for smiles in seq] # If we pass a string, tokenizer will smartly think we want to create a sequence for each symbol\n",
    "    tokenized = tokenizer(seq, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "    return tokenized\n",
    "\n",
    "def load_model(path):\n",
    "    config = AutoConfig.from_pretrained('/kaggle/input/smiles-deberta77m-tokenizer')\n",
    "    model = CustomModel(config).cuda()\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_predictions(model, scaler, smiles_seq):\n",
    "    aggregated_preds = []\n",
    "    for smiles in smiles_seq:\n",
    "        smiles = [smiles]\n",
    "        smiles_tokenized = tokenize_smiles(smiles)\n",
    "\n",
    "        input_ids = smiles_tokenized['input_ids'].cuda()\n",
    "        attention_mask = smiles_tokenized['attention_mask'].cuda()\n",
    "        with torch.no_grad():\n",
    "            preds = model(input_ids=input_ids, scaler=scaler, attention_mask=attention_mask)['logits'].cpu().numpy()\n",
    "        \n",
    "        true_preds = scaler.inverse_transform(preds).flatten()\n",
    "        aggregated_preds.append(true_preds.tolist())\n",
    "    return np.array(aggregated_preds)\n",
    "\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')\n",
    "test_copy = test.copy()\n",
    "\n",
    "smiles_test = test['SMILES'].to_list()\n",
    "\n",
    "targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "scalers = joblib.load('/kaggle/input/smiles-bert-models/target_scalers.pkl')\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/smiles-deberta77m-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import random\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "def augment_smiles_dataset(df: pd.DataFrame,\n",
    "                               smiles_column: str = 'SMILES',\n",
    "                               augmentation_strategies: List[str] = ['enumeration', 'kekulize', 'stereo_enum'],\n",
    "                               n_augmentations: int = 100,\n",
    "                               preserve_original: bool = True,\n",
    "                               random_seed: Optional[int] = None) -> pd.DataFrame:\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    def apply_augmentation_strategy(smiles: str, strategy: str) -> List[str]:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                return [smiles]\n",
    "            \n",
    "            augmented = []\n",
    "            \n",
    "            if strategy == 'enumeration':\n",
    "                # Standard SMILES enumeration\n",
    "                for _ in range(n_augmentations):\n",
    "                    enum_smiles = Chem.MolToSmiles(mol, \n",
    "                                                 canonical=False, \n",
    "                                                 doRandom=True,\n",
    "                                                 isomericSmiles=True)\n",
    "                    augmented.append(enum_smiles)\n",
    "            \n",
    "            elif strategy == 'kekulize':\n",
    "                # Kekulization variants\n",
    "                try:\n",
    "                    Chem.Kekulize(mol)\n",
    "                    kek_smiles = Chem.MolToSmiles(mol, kekuleSmiles=True)\n",
    "                    augmented.append(kek_smiles)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'stereo_enum':\n",
    "                # Stereochemistry enumeration\n",
    "                for _ in range(n_augmentations // 2):\n",
    "                    # Remove stereochemistry\n",
    "                    Chem.RemoveStereochemistry(mol)\n",
    "                    no_stereo = Chem.MolToSmiles(mol)\n",
    "                    augmented.append(no_stereo)\n",
    "            \n",
    "            return list(set(augmented))  # Remove duplicates\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in {strategy} for {smiles}: {e}\")\n",
    "            return [smiles]\n",
    "    \n",
    "    augmented_rows = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        original_smiles = row[smiles_column]\n",
    "        \n",
    "        if preserve_original:\n",
    "            original_row = row.to_dict()\n",
    "            original_row['augmentation_strategy'] = 'original'\n",
    "            original_row['is_augmented'] = False\n",
    "            augmented_rows.append(original_row)\n",
    "        \n",
    "        for strategy in augmentation_strategies:\n",
    "            strategy_smiles = apply_augmentation_strategy(original_smiles, strategy)\n",
    "            \n",
    "            for aug_smiles in strategy_smiles:\n",
    "                if aug_smiles != original_smiles:\n",
    "                    new_row = row.to_dict().copy()\n",
    "                    new_row[smiles_column] = aug_smiles\n",
    "                    new_row['augmentation_strategy'] = strategy\n",
    "                    new_row['is_augmented'] = True\n",
    "                    augmented_rows.append(new_row)\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    augmented_df = augmented_df.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Original size: {len(df)}, Augmented size: {len(augmented_df)}\")\n",
    "    print(f\"Augmentation factor: {len(augmented_df) / len(df):.2f}x\")\n",
    "    \n",
    "    return augmented_df\n",
    "\n",
    "test = augment_smiles_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mapping = {}\n",
    "\n",
    "for i in tqdm(range(len(targets))):\n",
    "    target = targets[i]\n",
    "    scaler = scalers[i]\n",
    "\n",
    "    model_path = f'/kaggle/input/smiles-bert-models/trained_smiles_model_{target}_target.pth' # Very sophisticated staff\n",
    "    model = load_model(model_path)\n",
    "    true_preds = []\n",
    "\n",
    "    for i, data in test.groupby('id'):\n",
    "        test_smiles = data['SMILES'].to_list()\n",
    "        augmented_preds = make_predictions(model, scaler, test_smiles)\n",
    "    \n",
    "        average_pred = np.median(augmented_preds)\n",
    "    \n",
    "        true_preds.append(float(average_pred.flatten()[0]))\n",
    "\n",
    "    preds_mapping[target] = true_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(preds_mapping)\n",
    "submission['id'] = test_copy['id']\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training notebook: [here](https://www.kaggle.com/code/defdet/polymer-bert-train?scriptVersionId=246123151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-06-19T14:40:43.987106Z",
     "iopub.status.busy": "2025-06-19T14:40:43.986738Z",
     "iopub.status.idle": "2025-06-19T14:40:47.612426Z",
     "shell.execute_reply": "2025-06-19T14:40:47.611570Z",
     "shell.execute_reply.started": "2025-06-19T14:40:43.987070Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (11.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit==2025.3.3) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit==2025.3.3) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit==2025.3.3) (2024.2.0)\n",
      "rdkit is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T14:40:47.614752Z",
     "iopub.status.busy": "2025-06-19T14:40:47.614462Z",
     "iopub.status.idle": "2025-06-19T14:40:47.625988Z",
     "shell.execute_reply": "2025-06-19T14:40:47.625166Z",
     "shell.execute_reply.started": "2025-06-19T14:40:47.614726Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from transformers import PreTrainedModel, AutoConfig, BertModel, BertTokenizerFast, BertConfig, AutoModel, AutoTokenizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torch import nn\n",
    "from transformers.activations import ACT2FN\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class ContextPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        pooler_size = getattr(config, 'pooler_hidden_size', config.hidden_size)\n",
    "        self.dense = nn.Linear(pooler_size, pooler_size)\n",
    "        \n",
    "        dropout_prob = getattr(config, 'pooler_dropout', config.hidden_dropout_prob)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.activation = getattr(config, 'pooler_hidden_act', config.hidden_act)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        context_token = hidden_states[:, 0] # CLS token\n",
    "        context_token = self.dropout(context_token)\n",
    "        pooled_output = self.dense(context_token)\n",
    "        pooled_output = ACT2FN[self.activation](pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class CustomModel(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.backbone = AutoModel.from_config(config)\n",
    "        \n",
    "        self.pooler = ContextPooler(config)\n",
    "\n",
    "        pooler_output_dim = getattr(config, 'pooler_hidden_size', config.hidden_size)\n",
    "        self.output = torch.nn.Linear(pooler_output_dim, 1) # Still predicting one label at a time. Kinda stupid\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        scaler,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.backbone(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "\n",
    "        pooled_output = self.pooler(outputs.last_hidden_state)\n",
    "        \n",
    "        # Final regression output\n",
    "        regression_output = self.output(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        true_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "            unscaled_labels = scaler.inverse_transform(labels.cpu().numpy())\n",
    "            unscaled_outputs = scaler.inverse_transform(regression_output.cpu().detach().numpy())\n",
    "            \n",
    "            loss = loss_fn(regression_output, labels)\n",
    "            true_loss = mean_absolute_error(unscaled_outputs, unscaled_labels)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": regression_output,\n",
    "            \"true_loss\": true_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-19T14:40:47.627080Z",
     "iopub.status.busy": "2025-06-19T14:40:47.626834Z",
     "iopub.status.idle": "2025-06-19T14:40:47.692675Z",
     "shell.execute_reply": "2025-06-19T14:40:47.691477Z",
     "shell.execute_reply.started": "2025-06-19T14:40:47.627060Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35/2779921138.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maugmented_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_smiles_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "def tokenize_smiles(seq):\n",
    "    seq = [tokenizer.cls_token + smiles for smiles in seq] # If we pass a string, tokenizer will smartly think we want to create a sequence for each symbol\n",
    "    tokenized = tokenizer(seq, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "    return tokenized\n",
    "\n",
    "def load_model(path):\n",
    "    config = AutoConfig.from_pretrained('/kaggle/input/smiles-deberta77m-tokenizer')\n",
    "    model = CustomModel(config).cuda()\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_predictions(model, scaler, smiles_seq):\n",
    "    aggregated_preds = []\n",
    "    for smiles in smiles_seq:\n",
    "        smiles = [smiles]\n",
    "        smiles_tokenized = tokenize_smiles(smiles)\n",
    "\n",
    "        input_ids = smiles_tokenized['input_ids'].cuda()\n",
    "        attention_mask = smiles_tokenized['attention_mask'].cuda()\n",
    "        with torch.no_grad():\n",
    "            preds = model(input_ids=input_ids, scaler=scaler, attention_mask=attention_mask)['logits'].cpu().numpy()\n",
    "        \n",
    "        true_preds = scaler.inverse_transform(preds).flatten()\n",
    "        aggregated_preds.append(true_preds.tolist())\n",
    "    return np.array(aggregated_preds)\n",
    "\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')\n",
    "test_copy = test.copy()\n",
    "\n",
    "smiles_test = test['SMILES'].to_list()\n",
    "\n",
    "targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "scalers = joblib.load('/kaggle/input/smiles-bert-models/target_scalers.pkl')\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/smiles-deberta77m-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import random\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "def augment_smiles_dataset(df: pd.DataFrame,\n",
    "                               smiles_column: str = 'SMILES',\n",
    "                               augmentation_strategies: List[str] = ['enumeration', 'kekulize', 'stereo_enum'],\n",
    "                               n_augmentations: int = 100,\n",
    "                               preserve_original: bool = True,\n",
    "                               random_seed: Optional[int] = None) -> pd.DataFrame:\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    def apply_augmentation_strategy(smiles: str, strategy: str) -> List[str]:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                return [smiles]\n",
    "            \n",
    "            augmented = []\n",
    "            \n",
    "            if strategy == 'enumeration':\n",
    "                # Standard SMILES enumeration\n",
    "                for _ in range(n_augmentations):\n",
    "                    enum_smiles = Chem.MolToSmiles(mol, \n",
    "                                                 canonical=False, \n",
    "                                                 doRandom=True,\n",
    "                                                 isomericSmiles=True)\n",
    "                    augmented.append(enum_smiles)\n",
    "            \n",
    "            elif strategy == 'kekulize':\n",
    "                # Kekulization variants\n",
    "                try:\n",
    "                    Chem.Kekulize(mol)\n",
    "                    kek_smiles = Chem.MolToSmiles(mol, kekuleSmiles=True)\n",
    "                    augmented.append(kek_smiles)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'stereo_enum':\n",
    "                # Stereochemistry enumeration\n",
    "                for _ in range(n_augmentations // 2):\n",
    "                    # Remove stereochemistry\n",
    "                    Chem.RemoveStereochemistry(mol)\n",
    "                    no_stereo = Chem.MolToSmiles(mol)\n",
    "                    augmented.append(no_stereo)\n",
    "            \n",
    "            return list(set(augmented))  # Remove duplicates\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in {strategy} for {smiles}: {e}\")\n",
    "            return [smiles]\n",
    "    \n",
    "    augmented_rows = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        original_smiles = row[smiles_column]\n",
    "        \n",
    "        if preserve_original:\n",
    "            original_row = row.to_dict()\n",
    "            original_row['augmentation_strategy'] = 'original'\n",
    "            original_row['is_augmented'] = False\n",
    "            augmented_rows.append(original_row)\n",
    "        \n",
    "        for strategy in augmentation_strategies:\n",
    "            strategy_smiles = apply_augmentation_strategy(original_smiles, strategy)\n",
    "            \n",
    "            for aug_smiles in strategy_smiles:\n",
    "                if aug_smiles != original_smiles:\n",
    "                    new_row = row.to_dict().copy()\n",
    "                    new_row[smiles_column] = aug_smiles\n",
    "                    new_row['augmentation_strategy'] = strategy\n",
    "                    new_row['is_augmented'] = True\n",
    "                    augmented_rows.append(new_row)\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    augmented_df = augmented_df.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Original size: {len(df)}, Augmented size: {len(augmented_df)}\")\n",
    "    print(f\"Augmentation factor: {len(augmented_df) / len(df):.2f}x\")\n",
    "    \n",
    "    return augmented_df\n",
    "\n",
    "test = augment_smiles_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mapping = {}\n",
    "\n",
    "for i in tqdm(range(len(targets))):\n",
    "    target = targets[i]\n",
    "    scaler = scalers[i]\n",
    "\n",
    "    model_path = f'/kaggle/input/smiles-bert-models/trained_smiles_model_{target}_target.pth' # Very sophisticated staff\n",
    "    model = load_model(model_path)\n",
    "    true_preds = []\n",
    "\n",
    "    for i, data in test.groupby('id'):\n",
    "        test_smiles = data['SMILES'].to_list()\n",
    "        augmented_preds = make_predictions(model, scaler, test_smiles)\n",
    "    \n",
    "        average_pred = np.median(augmented_preds)\n",
    "    \n",
    "        true_preds.append(float(average_pred.flatten()[0]))\n",
    "\n",
    "    preds_mapping[target] = true_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-19T14:40:47.696410Z",
     "iopub.status.idle": "2025-06-19T14:40:47.696818Z",
     "shell.execute_reply": "2025-06-19T14:40:47.696598Z",
     "shell.execute_reply.started": "2025-06-19T14:40:47.696583Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(preds_mapping)\n",
    "submission['id'] = test_copy['id']\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "增强SMILES: 100%|██████████| 3/3 [00:00<00:00,  5.36it/s]\n",
    "\n",
    "增强结果统计:\n",
    "原始数据集大小: 3\n",
    "增强后数据集大小: 3\n",
    "总体增强倍数: 1.00x\n",
    "\n",
    "各策略贡献:\n",
    "- enumeration: 0条记录 (0.00x)\n",
    "- kekulize: 0条记录 (0.00x)\n",
    "- stereo_enum: 0条记录 (0.00x)\n",
    "- fragment: 0条记录 (0.00x)\n",
    "- rotate: 0条记录 (0.00x)\n",
    "- original: 3条记录 (1.00x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "from rdkit.Chem import BRICS\n",
    "import random\n",
    "from typing import Optional, List, Union, Dict, Any\n",
    "from tqdm import tqdm\n",
    "\n",
    "def augment_smiles_dataset(df: pd.DataFrame,\n",
    "                          smiles_column: str = 'SMILES',\n",
    "                          augmentation_strategies: List[str] = ['enumeration', 'kekulize', 'stereo_enum', 'fragment', 'rotate'],\n",
    "                          n_augmentations: int = 100,\n",
    "                          preserve_original: bool = True,\n",
    "                          random_seed: Optional[int] = None,\n",
    "                          max_fragment_combinations: int = 2) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    增强的SMILES数据集生成器\n",
    "    \n",
    "    参数:\n",
    "    - df: 输入的DataFrame\n",
    "    - smiles_column: SMILES列的名称\n",
    "    - augmentation_strategies: 增强策略列表\n",
    "    - n_augmentations: 每个策略的增强次数\n",
    "    - preserve_original: 是否保留原始SMILES\n",
    "    - random_seed: 随机种子\n",
    "    - max_fragment_combinations: 片段重组的最大组合数\n",
    "    \n",
    "    返回:\n",
    "    - 增强后的DataFrame\n",
    "    \"\"\"\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    def sanitize_smiles(smiles: str) -> str:\n",
    "        \"\"\"净化SMILES字符串\"\"\"\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is not None:\n",
    "                return Chem.MolToSmiles(mol, canonical=True)\n",
    "        except:\n",
    "            pass\n",
    "        return smiles\n",
    "    \n",
    "    def apply_augmentation_strategy(smiles: str, strategy: str) -> List[str]:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                return [smiles]\n",
    "            \n",
    "            augmented = set()  # 使用集合避免重复\n",
    "            \n",
    "            if strategy == 'enumeration':\n",
    "                # 增强的SMILES枚举\n",
    "                for _ in range(n_augmentations):\n",
    "                    enum_smiles = Chem.MolToSmiles(mol, \n",
    "                                                 canonical=False, \n",
    "                                                 doRandom=True,\n",
    "                                                 isomericSmiles=True)\n",
    "                    augmented.add(sanitize_smiles(enum_smiles))\n",
    "            \n",
    "            elif strategy == 'kekulize':\n",
    "                # Kekulization变体\n",
    "                try:\n",
    "                    Chem.Kekulize(mol)\n",
    "                    kek_smiles = Chem.MolToSmiles(mol, kekuleSmiles=True)\n",
    "                    augmented.add(sanitize_smiles(kek_smiles))\n",
    "                    \n",
    "                    # 添加不同的Kekulization形式\n",
    "                    for _ in range(min(5, n_augmentations)):\n",
    "                        new_mol = Chem.MolFromSmiles(kek_smiles)\n",
    "                        if new_mol:\n",
    "                            Chem.Kekulize(new_mol)\n",
    "                            new_smiles = Chem.MolToSmiles(new_mol, kekuleSmiles=True, doRandom=True)\n",
    "                            augmented.add(sanitize_smiles(new_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'stereo_enum':\n",
    "                # 立体化学枚举\n",
    "                try:\n",
    "                    # 移除立体化学\n",
    "                    Chem.RemoveStereochemistry(mol)\n",
    "                    no_stereo = Chem.MolToSmiles(mol)\n",
    "                    augmented.add(sanitize_smiles(no_stereo))\n",
    "                    \n",
    "                    # 添加随机立体中心\n",
    "                    for _ in range(n_augmentations // 2):\n",
    "                        new_mol = Chem.MolFromSmiles(no_stereo)\n",
    "                        if new_mol:\n",
    "                            Chem.AssignStereochemistry(new_mol, force=True, cleanIt=True)\n",
    "                            stereo_smiles = Chem.MolToSmiles(new_mol, isomericSmiles=True)\n",
    "                            augmented.add(sanitize_smiles(stereo_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'fragment':\n",
    "                # BRICS分解和重组\n",
    "                try:\n",
    "                    fragments = list(BRICS.BRICSDecompose(mol))\n",
    "                    if len(fragments) > 1:\n",
    "                        # 随机组合片段\n",
    "                        for _ in range(min(max_fragment_combinations, n_augmentations)):\n",
    "                            n_frags = random.randint(2, min(len(fragments), 3))\n",
    "                            selected_frags = random.sample(fragments, n_frags)\n",
    "                            new_mol = BRICS.BRICSBuild([Chem.MolFromSmiles(f) for f in selected_frags])\n",
    "                            if new_mol:\n",
    "                                new_smiles = Chem.MolToSmiles(new_mol)\n",
    "                                augmented.add(sanitize_smiles(new_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'rotate':\n",
    "                # 原子顺序旋转\n",
    "                try:\n",
    "                    for _ in range(n_augmentations // 2):\n",
    "                        rot_smiles = Chem.MolToSmiles(mol, \n",
    "                                                     doRandom=True,\n",
    "                                                     canonical=False,\n",
    "                                                     rootedAtAtom=random.randint(0, mol.GetNumAtoms()-1))\n",
    "                        augmented.add(sanitize_smiles(rot_smiles))\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            return list(augmented)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"警告: {strategy}处理{smiles}时出错: {e}\")\n",
    "            return [smiles]\n",
    "    \n",
    "    augmented_rows = []\n",
    "    stats: Dict[str, int] = {strategy: 0 for strategy in augmentation_strategies}\n",
    "    stats['original'] = 0\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"增强SMILES\"):\n",
    "        original_smiles = row[smiles_column]\n",
    "        \n",
    "        if preserve_original:\n",
    "            original_row = row.to_dict()\n",
    "            original_row['augmentation_strategy'] = 'original'\n",
    "            original_row['is_augmented'] = False\n",
    "            augmented_rows.append(original_row)\n",
    "            stats['original'] += 1\n",
    "        \n",
    "        for strategy in augmentation_strategies:\n",
    "            strategy_smiles = apply_augmentation_strategy(original_smiles, strategy)\n",
    "            \n",
    "            for aug_smiles in strategy_smiles:\n",
    "                if aug_smiles != original_smiles:\n",
    "                    new_row = row.to_dict().copy()\n",
    "                    new_row[smiles_column] = aug_smiles\n",
    "                    new_row['augmentation_strategy'] = strategy\n",
    "                    new_row['is_augmented'] = True\n",
    "                    augmented_rows.append(new_row)\n",
    "                    stats[strategy] += 1\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    augmented_df = augmented_df.reset_index(drop=True)\n",
    "    \n",
    "    # 打印详细统计信息\n",
    "    print(\"\\n增强结果统计:\")\n",
    "    print(f\"原始数据集大小: {len(df)}\")\n",
    "    print(f\"增强后数据集大小: {len(augmented_df)}\")\n",
    "    print(f\"总体增强倍数: {len(augmented_df) / len(df):.2f}x\")\n",
    "    print(\"\\n各策略贡献:\")\n",
    "    for strategy, count in stats.items():\n",
    "        print(f\"- {strategy}: {count}条记录 ({count/len(df):.2f}x)\")\n",
    "    \n",
    "    return augmented_df\n",
    "\n",
    "test = augment_smiles_dataset(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (1.26.4)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from rdkit==2025.3.3) (11.1.0)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2025.1.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2022.1.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->rdkit==2025.3.3) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->rdkit==2025.3.3) (2022.1.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->rdkit==2025.3.3) (1.3.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->rdkit==2025.3.3) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->rdkit==2025.3.3) (2024.2.0)\n",
      "rdkit is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n"
     ]
    }
   ],
   "source": [
    "!pip install /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from transformers import PreTrainedModel, AutoConfig, BertModel, BertTokenizerFast, BertConfig, AutoModel, AutoTokenizer\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from torch import nn\n",
    "from transformers.activations import ACT2FN\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "class ContextPooler(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        pooler_size = getattr(config, 'pooler_hidden_size', config.hidden_size)\n",
    "        self.dense = nn.Linear(pooler_size, pooler_size)\n",
    "        \n",
    "        dropout_prob = getattr(config, 'pooler_dropout', config.hidden_dropout_prob)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.activation = getattr(config, 'pooler_hidden_act', config.hidden_act)\n",
    "        self.config = config\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        context_token = hidden_states[:, 0] # CLS token\n",
    "        context_token = self.dropout(context_token)\n",
    "        pooled_output = self.dense(context_token)\n",
    "        pooled_output = ACT2FN[self.activation](pooled_output)\n",
    "        return pooled_output\n",
    "\n",
    "class CustomModel(PreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.backbone = AutoModel.from_config(config)\n",
    "        \n",
    "        self.pooler = ContextPooler(config)\n",
    "\n",
    "        pooler_output_dim = getattr(config, 'pooler_hidden_size', config.hidden_size)\n",
    "        self.output = torch.nn.Linear(pooler_output_dim, 1) # Still predicting one label at a time. Kinda stupid\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids,\n",
    "        scaler,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        labels=None,\n",
    "    ):\n",
    "        outputs = self.backbone(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "        )\n",
    "\n",
    "        pooled_output = self.pooler(outputs.last_hidden_state)\n",
    "        \n",
    "        # Final regression output\n",
    "        regression_output = self.output(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        true_loss = None\n",
    "        if labels is not None:\n",
    "            loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "            unscaled_labels = scaler.inverse_transform(labels.cpu().numpy())\n",
    "            unscaled_outputs = scaler.inverse_transform(regression_output.cpu().detach().numpy())\n",
    "            \n",
    "            loss = loss_fn(regression_output, labels)\n",
    "            true_loss = mean_absolute_error(unscaled_outputs, unscaled_labels)\n",
    "\n",
    "        return {\n",
    "            \"loss\": loss,\n",
    "            \"logits\": regression_output,\n",
    "            \"true_loss\": true_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)\n",
      "\u001b[0;32m/tmp/ipykernel_35/2779921138.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m     85\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maugmented_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m---> 87\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maugment_smiles_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test' is not defined"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 16\n",
    "\n",
    "def tokenize_smiles(seq):\n",
    "    seq = [tokenizer.cls_token + smiles for smiles in seq] # If we pass a string, tokenizer will smartly think we want to create a sequence for each symbol\n",
    "    tokenized = tokenizer(seq, padding='max_length', truncation=True, max_length=512, return_tensors='pt')\n",
    "    return tokenized\n",
    "\n",
    "def load_model(path):\n",
    "    config = AutoConfig.from_pretrained('/kaggle/input/smiles-deberta77m-tokenizer')\n",
    "    model = CustomModel(config).cuda()\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    return model\n",
    "\n",
    "\n",
    "def make_predictions(model, scaler, smiles_seq):\n",
    "    aggregated_preds = []\n",
    "    for smiles in smiles_seq:\n",
    "        smiles = [smiles]\n",
    "        smiles_tokenized = tokenize_smiles(smiles)\n",
    "\n",
    "        input_ids = smiles_tokenized['input_ids'].cuda()\n",
    "        attention_mask = smiles_tokenized['attention_mask'].cuda()\n",
    "        with torch.no_grad():\n",
    "            preds = model(input_ids=input_ids, scaler=scaler, attention_mask=attention_mask)['logits'].cpu().numpy()\n",
    "        \n",
    "        true_preds = scaler.inverse_transform(preds).flatten()\n",
    "        aggregated_preds.append(true_preds.tolist())\n",
    "    return np.array(aggregated_preds)\n",
    "\n",
    "\n",
    "test = pd.read_csv('/kaggle/input/neurips-open-polymer-prediction-2025/test.csv')\n",
    "test_copy = test.copy()\n",
    "\n",
    "smiles_test = test['SMILES'].to_list()\n",
    "\n",
    "targets = ['Tg', 'FFV', 'Tc', 'Density', 'Rg']\n",
    "\n",
    "scalers = joblib.load('/kaggle/input/smiles-bert-models/target_scalers.pkl')\n",
    "tokenizer = AutoTokenizer.from_pretrained('/kaggle/input/smiles-deberta77m-tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from rdkit import Chem\n",
    "import random\n",
    "from typing import Optional, List, Union\n",
    "\n",
    "def augment_smiles_dataset(df: pd.DataFrame,\n",
    "                               smiles_column: str = 'SMILES',\n",
    "                               augmentation_strategies: List[str] = ['enumeration', 'kekulize', 'stereo_enum'],\n",
    "                               n_augmentations: int = 100,\n",
    "                               preserve_original: bool = True,\n",
    "                               random_seed: Optional[int] = None) -> pd.DataFrame:\n",
    "    if random_seed is not None:\n",
    "        random.seed(random_seed)\n",
    "        np.random.seed(random_seed)\n",
    "    \n",
    "    def apply_augmentation_strategy(smiles: str, strategy: str) -> List[str]:\n",
    "        try:\n",
    "            mol = Chem.MolFromSmiles(smiles)\n",
    "            if mol is None:\n",
    "                return [smiles]\n",
    "            \n",
    "            augmented = []\n",
    "            \n",
    "            if strategy == 'enumeration':\n",
    "                # Standard SMILES enumeration\n",
    "                for _ in range(n_augmentations):\n",
    "                    enum_smiles = Chem.MolToSmiles(mol, \n",
    "                                                 canonical=False, \n",
    "                                                 doRandom=True,\n",
    "                                                 isomericSmiles=True)\n",
    "                    augmented.append(enum_smiles)\n",
    "            \n",
    "            elif strategy == 'kekulize':\n",
    "                # Kekulization variants\n",
    "                try:\n",
    "                    Chem.Kekulize(mol)\n",
    "                    kek_smiles = Chem.MolToSmiles(mol, kekuleSmiles=True)\n",
    "                    augmented.append(kek_smiles)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            elif strategy == 'stereo_enum':\n",
    "                # Stereochemistry enumeration\n",
    "                for _ in range(n_augmentations // 2):\n",
    "                    # Remove stereochemistry\n",
    "                    Chem.RemoveStereochemistry(mol)\n",
    "                    no_stereo = Chem.MolToSmiles(mol)\n",
    "                    augmented.append(no_stereo)\n",
    "            \n",
    "            return list(set(augmented))  # Remove duplicates\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error in {strategy} for {smiles}: {e}\")\n",
    "            return [smiles]\n",
    "    \n",
    "    augmented_rows = []\n",
    "    \n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        original_smiles = row[smiles_column]\n",
    "        \n",
    "        if preserve_original:\n",
    "            original_row = row.to_dict()\n",
    "            original_row['augmentation_strategy'] = 'original'\n",
    "            original_row['is_augmented'] = False\n",
    "            augmented_rows.append(original_row)\n",
    "        \n",
    "        for strategy in augmentation_strategies:\n",
    "            strategy_smiles = apply_augmentation_strategy(original_smiles, strategy)\n",
    "            \n",
    "            for aug_smiles in strategy_smiles:\n",
    "                if aug_smiles != original_smiles:\n",
    "                    new_row = row.to_dict().copy()\n",
    "                    new_row[smiles_column] = aug_smiles\n",
    "                    new_row['augmentation_strategy'] = strategy\n",
    "                    new_row['is_augmented'] = True\n",
    "                    augmented_rows.append(new_row)\n",
    "    \n",
    "    augmented_df = pd.DataFrame(augmented_rows)\n",
    "    augmented_df = augmented_df.reset_index(drop=True)\n",
    "    \n",
    "    print(f\"Original size: {len(df)}, Augmented size: {len(augmented_df)}\")\n",
    "    print(f\"Augmentation factor: {len(augmented_df) / len(df):.2f}x\")\n",
    "    \n",
    "    return augmented_df\n",
    "\n",
    "test = augment_smiles_dataset(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mapping = {}\n",
    "\n",
    "for i in tqdm(range(len(targets))):\n",
    "    target = targets[i]\n",
    "    scaler = scalers[i]\n",
    "\n",
    "    model_path = f'/kaggle/input/smiles-bert-models/trained_smiles_model_{target}_target.pth' # Very sophisticated staff\n",
    "    model = load_model(model_path)\n",
    "    true_preds = []\n",
    "\n",
    "    for i, data in test.groupby('id'):\n",
    "        test_smiles = data['SMILES'].to_list()\n",
    "        augmented_preds = make_predictions(model, scaler, test_smiles)\n",
    "    \n",
    "        average_pred = np.median(augmented_preds)\n",
    "    \n",
    "        true_preds.append(float(average_pred.flatten()[0]))\n",
    "\n",
    "    preds_mapping[target] = true_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.DataFrame(preds_mapping)\n",
    "submission['id'] = test_copy['id']\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training notebook: [here](https://www.kaggle.com/code/defdet/polymer-bert-train?scriptVersionId=246123151)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training notebook: [here](https://www.kaggle.com/code/defdet/polymer-bert-train?scriptVersionId=246123151)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 12609125,
     "sourceId": 74608,
     "sourceType": "competition"
    },
    {
     "datasetId": 7678100,
     "sourceId": 12189904,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7688334,
     "sourceId": 12204996,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7688411,
     "sourceId": 12205277,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
